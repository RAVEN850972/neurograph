
## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞

---

# üìã –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [–ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç](#–±—ã—Å—Ç—Ä—ã–π-—Å—Ç–∞—Ä—Ç)
2. [–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞](#—É—Å—Ç–∞–Ω–æ–≤–∫–∞-–∏-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞)
3. [–û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏](#–æ—Å–Ω–æ–≤–Ω—ã–µ-–∫–æ–Ω—Ü–µ–ø—Ü–∏–∏)
4. [API Reference](#api-reference)
5. [–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã](#–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ-–ø—Ä–∏–º–µ—Ä—ã)
6. [–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è](#–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è)
7. [–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è](#–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è)
8. [–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã](#—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ-—Å–∏—Å—Ç–µ–º—ã)
9. [–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ](#—Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ)
10. [Troubleshooting](#troubleshooting)

---

# üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

## 30-—Å–µ–∫—É–Ω–¥–Ω—ã–π –ø—Ä–∏–º–µ—Ä

```python
from neurograph.integration import create_default_engine

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–≤–∏–∂–∫–∞ NeuroGraph
engine = create_default_engine()

# –û–±—É—á–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
engine.learn("Python - —ç—Ç–æ —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –ò–ò")

# –ó–∞–ø—Ä–æ—Å –∫ —Å–∏—Å—Ç–µ–º–µ
response = engine.query("–ß—Ç–æ —Ç–∞–∫–æ–µ Python?")
print(response.primary_response)
# Output: "Python - —ç—Ç–æ —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞..."

# –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
engine.shutdown()
```

## –ß—Ç–æ –≤—ã –ø–æ–ª—É—á–∏—Ç–µ

‚úÖ **–ü–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç** –∑–∞ 4 —Å—Ç—Ä–æ–∫–∏ –∫–æ–¥–∞  
‚úÖ **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π** –∏–∑ —Ç–µ–∫—Å—Ç–∞  
‚úÖ **–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã** –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã  
‚úÖ **–ë–∏–æ–º–æ—Ä—Ñ–Ω—É—é –ø–∞–º—è—Ç—å** —Å –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–µ–π –∑–Ω–∞–Ω–∏–π  
‚úÖ **–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫** –∏ –∞—Å—Å–æ—Ü–∏–∞—Ç–∏–≤–Ω—ã–µ —Å–≤—è–∑–∏  

---

# üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

## –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

### –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
- **Python**: 3.8+
- **RAM**: 2GB
- **CPU**: 2 —è–¥—Ä–∞
- **–î–∏—Å–∫**: 1GB —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
- **Python**: 3.11+
- **RAM**: 8GB
- **CPU**: 4+ —è–¥—Ä–∞
- **–î–∏—Å–∫**: 5GB —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞
- **GPU**: –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

## –£—Å—Ç–∞–Ω–æ–≤–∫–∞

### –°–ø–æ—Å–æ–± 1: pip (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
```bash
# –ë–∞–∑–æ–≤–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞
pip install neurograph

# –° –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
pip install neurograph[full]

# –î–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
pip install neurograph[dev]
```

### –°–ø–æ—Å–æ–± 2: –∏–∑ –∏—Å—Ö–æ–¥–Ω–∏–∫–æ–≤
```bash
git clone https://github.com/neurograph/neurograph.git
cd neurograph
pip install -e .
```

### –°–ø–æ—Å–æ–± 3: Docker
```bash
docker pull neurograph/neurograph:latest
docker run -p 8080:8080 neurograph/neurograph:latest
```

## –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏

```python
# test_installation.py
from neurograph.integration import create_default_engine
from neurograph.core import get_version

print(f"NeuroGraph –≤–µ—Ä—Å–∏—è: {get_version()}")

# –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
engine = create_default_engine()
test_response = engine.query("–¢–µ—Å—Ç —Å–∏—Å—Ç–µ–º—ã")
print(f"–°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç: {test_response.success}")
engine.shutdown()
```

## –ù–∞—á–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
```bash
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∞–∑–æ–≤–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
neurograph init --config-type default

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞
neurograph init --config-type production

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
neurograph init --config-type development
```

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞
```
my-neurograph-app/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ default.json
‚îÇ   ‚îú‚îÄ‚îÄ development.json
‚îÇ   ‚îî‚îÄ‚îÄ production.json
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ knowledge_graph.json
‚îÇ   ‚îî‚îÄ‚îÄ memory_storage/
‚îú‚îÄ‚îÄ logs/
‚îî‚îÄ‚îÄ app.py
```

---

# üß† –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã

### –£—Ä–æ–≤–Ω–∏ –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Application   ‚îÇ  ‚Üê –í–∞—à–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Integration   ‚îÇ  ‚Üê –û—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è –º–æ–¥—É–ª–µ–π
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Processing    ‚îÇ  ‚Üê NLP, Processor, Propagation
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Knowledge     ‚îÇ  ‚Üê SemGraph, ContextVec, Memory
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ      Core       ‚îÇ  ‚Üê –ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### –ö–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã

#### NeuroGraphEngine
–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –¥–≤–∏–∂–æ–∫ —Å–∏—Å—Ç–µ–º—ã:
- **–ö–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç** –≤—Å–µ –º–æ–¥—É–ª–∏
- **–ú–∞—Ä—à—Ä—É—Ç–∏–∑–∏—Ä—É–µ—Ç** –∑–∞–ø—Ä–æ—Å—ã –∫ –Ω—É–∂–Ω—ã–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º
- **–£–ø—Ä–∞–≤–ª—è–µ—Ç** –∂–∏–∑–Ω–µ–Ω–Ω—ã–º —Ü–∏–∫–ª–æ–º —Å–∏—Å—Ç–µ–º—ã
- **–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç** –µ–¥–∏–Ω—ã–π API

#### –ú–æ–¥—É–ª–∏ –∑–Ω–∞–Ω–∏–π
- **SemGraph**: —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≥—Ä–∞—Ñ —Å–≤—è–∑–µ–π
- **ContextVec**: –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
- **Memory**: –±–∏–æ–º–æ—Ä—Ñ–Ω–∞—è –ø–∞–º—è—Ç—å —Å –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–µ–π

#### –ú–æ–¥—É–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
- **NLP**: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ –æ—Ç–Ω–æ—à–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞
- **Processor**: –ª–æ–≥–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
- **Propagation**: —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –ø–æ –≥—Ä–∞—Ñ—É

## –ü–æ—Ç–æ–∫–∏ –¥–∞–Ω–Ω—ã—Ö

### –ü–æ—Ç–æ–∫ –æ–±—É—á–µ–Ω–∏—è
```
–¢–µ–∫—Å—Ç ‚Üí NLP ‚Üí SemGraph ‚Üí ContextVec ‚Üí Memory
      ‚Üì       ‚Üì           ‚Üì           ‚Üì
   –°—É—â–Ω–æ—Å—Ç–∏ –£–∑–ª—ã      –í–µ–∫—Ç–æ—Ä—ã    –≠–ª–µ–º–µ–Ω—Ç—ã
            –°–≤—è–∑–∏                  –ø–∞–º—è—Ç–∏
```

### –ü–æ—Ç–æ–∫ –∑–∞–ø—Ä–æ—Å–∞
```
–í–æ–ø—Ä–æ—Å ‚Üí Analysis ‚Üí Multi-Search ‚Üí Inference ‚Üí Response
                    ‚Üì            ‚Üì           ‚Üì
                  Graph        Memory    Processor
                 ContextVec   Propagation
```

---

# üìö API Reference

## –û—Å–Ω–æ–≤–Ω–æ–π API (NeuroGraphEngine)

### –°–æ–∑–¥–∞–Ω–∏–µ –¥–≤–∏–∂–∫–∞

```python
from neurograph.integration import (
    create_default_engine,
    create_lightweight_engine,
    create_research_engine,
    create_production_engine
)

# –†–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
engine = create_default_engine()           # –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è
engine = create_lightweight_engine()       # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã
engine = create_research_engine()          # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
engine = create_production_engine()        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞

# –°–æ–∑–¥–∞–Ω–∏–µ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
from neurograph.integration import NeuroGraphEngine, IntegrationConfig

config = IntegrationConfig.load_from_file("my_config.json")
engine = NeuroGraphEngine(config)
```

### –ë–∞–∑–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏

```python
# –û–±—É—á–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
response = engine.learn(
    content="–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∏–∑—É—á–∞–µ—Ç –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
    source="user_input",           # –ò—Å—Ç–æ—á–Ω–∏–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    importance=1.0,                # –í–∞–∂–Ω–æ—Å—Ç—å (0.0-1.0)
    tags=["AI", "ML"]              # –¢–µ–≥–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
)

# –ó–∞–ø—Ä–æ—Å –∫ —Å–∏—Å—Ç–µ–º–µ
response = engine.query(
    question="–ß—Ç–æ –∏–∑—É—á–∞–µ—Ç –ò–ò?",
    context={"domain": "technology"},  # –ö–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
    max_results=5,                     # –ú–∞–∫—Å–∏–º—É–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    include_explanations=True          # –í–∫–ª—é—á–∏—Ç—å –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
)

# –õ–æ–≥–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥
response = engine.infer(
    premises=["–í—Å–µ –ø—Ç–∏—Ü—ã –ª–µ—Ç–∞—é—Ç", "–ü–∏–Ω–≥–≤–∏–Ω - –ø—Ç–∏—Ü–∞"],
    max_depth=3                    # –ì–ª—É–±–∏–Ω–∞ –≤—ã–≤–æ–¥–∞
)

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
response = engine.process_text(
    text="–°—Ç–∞—Ç—å—è –æ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö...",
    extract_entities=True,        # –ò–∑–≤–ª–µ—á—å —Å—É—â–Ω–æ—Å—Ç–∏
    extract_relations=True,       # –ò–∑–≤–ª–µ—á—å –æ—Ç–Ω–æ—à–µ–Ω–∏—è
    create_summary=True           # –°–æ–∑–¥–∞—Ç—å —Ä–µ–∑—é–º–µ
)
```

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–æ–≤

```python
# ProcessingResponse - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞
class ProcessingResponse:
    success: bool                          # –£—Å–ø–µ—à–Ω–æ—Å—Ç—å –æ–ø–µ—Ä–∞—Ü–∏–∏
    primary_response: str                  # –û—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç
    confidence: float                      # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (0.0-1.0)
    processing_time: float                 # –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Å–µ–∫)
    
    # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç –º–æ–¥—É–ª–µ–π
    structured_data: Dict[str, Any] = {
        'nlp_result': {...},               # –†–µ–∑—É–ª—å—Ç–∞—Ç NLP –æ–±—Ä–∞–±–æ—Ç–∫–∏
        'graph_data': {...},               # –î–∞–Ω–Ω—ã–µ –∏–∑ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
        'memory_matches': [...],           # –ù–∞–π–¥–µ–Ω–Ω—ã–µ –≤ –ø–∞–º—è—Ç–∏ —ç–ª–µ–º–µ–Ω—Ç—ã
        'inference_chain': [...],          # –¶–µ–ø–æ—á–∫–∞ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–≤–æ–¥–∞
        'propagation_result': {...}       # –†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
    }
    
    # –ú–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    explanation: List[str]                 # –û–±—ä—è—Å–Ω–µ–Ω–∏—è –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    sources: List[str]                     # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    related_concepts: List[str]            # –°–≤—è–∑–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã
    error_message: Optional[str]           # –°–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
```

## –†–∞–±–æ—Ç–∞ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –º–æ–¥—É–ª—è–º–∏

### SemGraph API

```python
# –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–∞ –∫ –≥—Ä–∞—Ñ—É –∑–Ω–∞–Ω–∏–π
graph = engine.get_component('semgraph')

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —É–∑–ª–æ–≤ –∏ —Å–≤—è–∑–µ–π
graph.add_node("Python", type="programming_language", popularity="high")
graph.add_edge("Python", "AI", "used_for", weight=0.9)

# –ü–æ–∏—Å–∫ –≤ –≥—Ä–∞—Ñ–µ
neighbors = graph.get_neighbors("Python")
path = graph.find_shortest_path("Python", "Machine Learning")

# –ê–Ω–∞–ª–∏–∑ –≥—Ä–∞—Ñ–∞
central_nodes = graph.get_central_nodes(top_n=10)
communities = graph.find_communities()
```

### Memory API

```python
# –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–∞ –∫ –ø–∞–º—è—Ç–∏
memory = engine.get_component('memory')

# –†–∞–±–æ—Ç–∞ —Å —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ –ø–∞–º—è—Ç–∏
recent_items = memory.get_recent_items(hours=24)
important_memories = memory.get_most_accessed_items(limit=10)

# –ü–æ–∏—Å–∫ –≤ –ø–∞–º—è—Ç–∏
results = memory.search_by_content("–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", limit=5)
semantic_results = memory.search_by_vector(query_vector, limit=5)

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏
stats = memory.get_memory_statistics()
```

### ContextVec API

```python
# –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–∞ –∫ –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º
vectors = engine.get_component('contextvec')

# –†–∞–±–æ—Ç–∞ —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏
vector = vectors.get_vector("machine_learning")
similar = vectors.get_most_similar("artificial_intelligence", top_n=5)

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
vectors.create_vector("new_concept", embedding_array)
```

### NLP API

```python
# –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–∞ –∫ NLP –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—É
nlp = engine.get_component('nlp')

# –ü—Ä—è–º–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
result = nlp.process_text(
    "–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏",
    extract_entities=True,
    extract_relations=True
)

# –î–æ—Å—Ç—É–ø –∫ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º
for entity in result.entities:
    print(f"–°—É—â–Ω–æ—Å—Ç—å: {entity.text}, –¢–∏–ø: {entity.entity_type}")

for relation in result.relations:
    print(f"–û—Ç–Ω–æ—à–µ–Ω–∏–µ: {relation.subject.text} -> {relation.predicate} -> {relation.object.text}")
```

---

# üí° –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã

## –ü—Ä–∏–º–µ—Ä 1: –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –∑–Ω–∞–Ω–∏–π

```python
from neurograph.integration import create_default_engine
import json

class PersonalKnowledgeAssistant:
    def __init__(self, user_name="User"):
        self.engine = create_default_engine()
        self.user_name = user_name
        self.context = {"user": user_name}
        
    def add_personal_fact(self, fact, category="general"):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ª–∏—á–Ω–æ–≥–æ —Ñ–∞–∫—Ç–∞"""
        response = self.engine.learn(
            content=f"{self.user_name}: {fact}",
            source="personal",
            tags=[category, "personal"],
            importance=0.8
        )
        return f"–ó–∞–ø–æ–º–Ω–∏–ª: {fact}"
    
    def ask_question(self, question):
        """–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É"""
        response = self.engine.query(
            question=question,
            context=self.context,
            include_explanations=True
        )
        
        return {
            "answer": response.primary_response,
            "confidence": response.confidence,
            "sources": response.sources,
            "related": response.related_concepts
        }
    
    def get_personal_insights(self):
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Å–∞–π—Ç—ã –æ –ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        # –ü–æ–∏—Å–∫ —Å–≤—è–∑–µ–π –≤ –ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        memory = self.engine.get_component('memory')
        personal_items = memory.search_by_tags(["personal"], limit=20)
        
        # –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ
        graph = self.engine.get_component('semgraph')
        user_node = graph.get_node(self.user_name)
        
        if user_node:
            connections = graph.get_neighbors(self.user_name)
            return {
                "total_facts": len(personal_items),
                "key_interests": connections[:5],
                "knowledge_areas": self._extract_categories(personal_items)
            }
    
    def _extract_categories(self, items):
        categories = {}
        for item in items:
            tags = item.metadata.get('tags', [])
            for tag in tags:
                if tag != 'personal':
                    categories[tag] = categories.get(tag, 0) + 1
        return sorted(categories.items(), key=lambda x: x[1], reverse=True)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
assistant = PersonalKnowledgeAssistant("–ê–ª–µ–∫—Å–µ–π")

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ª–∏—á–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤
assistant.add_personal_fact("–Ø —Ä–∞–±–æ—Ç–∞—é –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–º –≤ IT –∫–æ–º–ø–∞–Ω–∏–∏", "work")
assistant.add_personal_fact("–ú–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –∏–∑—É—á–∞—Ç—å –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "interests")
assistant.add_personal_fact("–£ –º–µ–Ω—è –µ—Å—Ç—å –∫–æ—Ç –ø–æ –∏–º–µ–Ω–∏ –ú—É—Ä–∑–∏–∫", "personal")

# –ó–∞–ø—Ä–æ—Å—ã
result = assistant.ask_question("–ß—Ç–æ —Ç—ã –∑–Ω–∞–µ—à—å –æ –º–æ–∏—Ö –∏–Ω—Ç–µ—Ä–µ—Å–∞—Ö?")
print(json.dumps(result, indent=2, ensure_ascii=False))

# –ò–Ω—Å–∞–π—Ç—ã
insights = assistant.get_personal_insights()
print("–í–∞—à –ø—Ä–æ—Ñ–∏–ª—å –∑–Ω–∞–Ω–∏–π:", insights)
```

## –ü—Ä–∏–º–µ—Ä 2: –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

```python
from neurograph.integration import create_research_engine
import os
from pathlib import Path

class DocumentAnalyzer:
    def __init__(self):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
        self.engine = create_research_engine()
        self.analysis_results = {}
    
    def analyze_document(self, file_path, document_type="general"):
        """–ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        
        # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        response = self.engine.process_text(
            text=content,
            extract_entities=True,
            extract_relations=True,
            create_summary=True
        )
        
        # –û–±—É—á–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        self.engine.learn(
            content=content,
            source=f"document:{os.path.basename(file_path)}",
            tags=[document_type, "document"],
            importance=0.9
        )
        
        # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        analysis = self._analyze_structure(response, content)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        doc_id = os.path.basename(file_path)
        self.analysis_results[doc_id] = analysis
        
        return analysis
    
    def _analyze_structure(self, response, content):
        """–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        
        nlp_data = response.structured_data.get('nlp_result', {})
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        entities = nlp_data.get('entities', [])
        relations = nlp_data.get('relations', [])
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π
        entity_types = {}
        for entity in entities:
            entity_type = entity.get('entity_type', 'UNKNOWN')
            entity_types[entity_type] = entity_types.get(entity_type, 0) + 1
        
        # –ê–Ω–∞–ª–∏–∑ –æ—Ç–Ω–æ—à–µ–Ω–∏–π
        relation_patterns = {}
        for relation in relations:
            rel_type = relation.get('predicate', 'unknown')
            relation_patterns[rel_type] = relation_patterns.get(rel_type, 0) + 1
        
        return {
            "summary": response.primary_response,
            "confidence": response.confidence,
            "statistics": {
                "total_entities": len(entities),
                "entity_distribution": entity_types,
                "total_relations": len(relations),
                "relation_patterns": relation_patterns,
                "text_length": len(content)
            },
            "key_entities": [e.get('text', '') for e in entities[:10]],
            "main_topics": self._extract_topics(entities),
            "processing_time": response.processing_time
        }
    
    def _extract_topics(self, entities):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π –ø–æ —Ç–∏–ø–∞–º –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Ç–µ–º
        topics = {}
        for entity in entities:
            entity_type = entity.get('entity_type', 'UNKNOWN')
            if entity_type not in topics:
                topics[entity_type] = []
            topics[entity_type].append(entity.get('text', ''))
        
        return {k: v[:5] for k, v in topics.items()}  # –¢–æ–ø-5 –ø–æ –∫–∞–∂–¥–æ–º—É —Ç–∏–ø—É
    
    def batch_analyze(self, directory_path, pattern="*.txt"):
        """–ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        
        directory = Path(directory_path)
        files = list(directory.glob(pattern))
        
        results = {}
        for file_path in files:
            try:
                print(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é: {file_path.name}")
                analysis = self.analyze_document(str(file_path))
                results[file_path.name] = analysis
                print(f"‚úÖ –ì–æ—Ç–æ–≤–æ: {file_path.name}")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ {file_path.name}: {e}")
                results[file_path.name] = {"error": str(e)}
        
        return results
    
    def compare_documents(self, doc1_id, doc2_id):
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤—É—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        
        if doc1_id not in self.analysis_results or doc2_id not in self.analysis_results:
            return {"error": "–û–¥–∏–Ω –∏–ª–∏ –æ–±–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"}
        
        doc1 = self.analysis_results[doc1_id]
        doc2 = self.analysis_results[doc2_id]
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
        vectors = self.engine.get_component('contextvec')
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω—ã)
        similarity = self._calculate_similarity(doc1, doc2)
        
        return {
            "similarity_score": similarity,
            "common_entities": self._find_common_entities(doc1, doc2),
            "common_topics": self._find_common_topics(doc1, doc2),
            "differences": self._find_differences(doc1, doc2)
        }
    
    def _calculate_similarity(self, doc1, doc2):
        """–†–∞—Å—á–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        # –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—é –∫–ª—é—á–µ–≤—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
        entities1 = set(doc1.get('key_entities', []))
        entities2 = set(doc2.get('key_entities', []))
        
        if len(entities1) == 0 and len(entities2) == 0:
            return 0.0
        
        intersection = len(entities1.intersection(entities2))
        union = len(entities1.union(entities2))
        
        return intersection / union if union > 0 else 0.0
    
    def _find_common_entities(self, doc1, doc2):
        entities1 = set(doc1.get('key_entities', []))
        entities2 = set(doc2.get('key_entities', []))
        return list(entities1.intersection(entities2))
    
    def _find_common_topics(self, doc1, doc2):
        topics1 = set(doc1.get('main_topics', {}).keys())
        topics2 = set(doc2.get('main_topics', {}).keys())
        return list(topics1.intersection(topics2))
    
    def _find_differences(self, doc1, doc2):
        entities1 = set(doc1.get('key_entities', []))
        entities2 = set(doc2.get('key_entities', []))
        
        return {
            "unique_to_doc1": list(entities1 - entities2),
            "unique_to_doc2": list(entities2 - entities1)
        }
    
    def generate_report(self, output_file="analysis_report.json"):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –ø–æ –≤—Å–µ–º –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"""
        
        report = {
            "total_documents": len(self.analysis_results),
            "analysis_timestamp": str(datetime.now()),
            "summary_statistics": self._calculate_summary_stats(),
            "documents": self.analysis_results
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        return report
    
    def _calculate_summary_stats(self):
        """–†–∞—Å—á–µ—Ç —Å–≤–æ–¥–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        if not self.analysis_results:
            return {}
        
        total_entities = sum(
            doc.get('statistics', {}).get('total_entities', 0) 
            for doc in self.analysis_results.values() 
            if 'statistics' in doc
        )
        
        total_relations = sum(
            doc.get('statistics', {}).get('total_relations', 0) 
            for doc in self.analysis_results.values() 
            if 'statistics' in doc
        )
        
        avg_confidence = sum(
            doc.get('confidence', 0) 
            for doc in self.analysis_results.values() 
            if 'confidence' in doc
        ) / len(self.analysis_results)
        
        return {
            "total_entities_extracted": total_entities,
            "total_relations_found": total_relations,
            "average_confidence": round(avg_confidence, 3),
            "documents_with_errors": len([
                doc for doc in self.analysis_results.values() 
                if 'error' in doc
            ])
        }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
from datetime import datetime

analyzer = DocumentAnalyzer()

# –ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
analysis = analyzer.analyze_document("research_paper.txt", "academic")
print("–ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞:")
print(f"–†–µ–∑—é–º–µ: {analysis['summary'][:200]}...")
print(f"–ù–∞–π–¥–µ–Ω–æ —Å—É—â–Ω–æ—Å—Ç–µ–π: {analysis['statistics']['total_entities']}")
print(f"–ù–∞–π–¥–µ–Ω–æ –æ—Ç–Ω–æ—à–µ–Ω–∏–π: {analysis['statistics']['total_relations']}")

# –ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
results = analyzer.batch_analyze("./documents/", "*.txt")
print(f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(results)}")

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
report = analyzer.generate_report("document_analysis_report.json")
```

## –ü—Ä–∏–º–µ—Ä 3: –ß–∞—Ç-–±–æ—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º

```python
from neurograph.integration import create_default_engine
from datetime import datetime, timedelta
import uuid

class ContextualChatBot:
    def __init__(self, bot_name="NeuroBot"):
        self.engine = create_default_engine()
        self.bot_name = bot_name
        self.conversations = {}  # user_id -> conversation_history
        
        # –ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è –±–æ—Ç–∞
        self._initialize_bot_knowledge()
    
    def _initialize_bot_knowledge(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑–æ–≤—ã—Ö –∑–Ω–∞–Ω–∏–π –±–æ—Ç–∞"""
        base_knowledge = [
            f"–ú–µ–Ω—è –∑–æ–≤—É—Ç {self.bot_name} –∏ —è –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ NeuroGraph",
            "–Ø –º–æ–≥—É –ø–æ–º–æ—á—å —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏, –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ –Ω–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –±–µ—Å–µ–¥—ã",
            "–Ø –∏—Å–ø–æ–ª—å–∑—É—é –Ω–µ–π—Ä–æ—Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤",
            "–ú–æ—è –ø–∞–º—è—Ç—å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –∫—Ä–∞—Ç–∫–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏ –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏, –∫–∞–∫ —É —á–µ–ª–æ–≤–µ–∫–∞"
        ]
        
        for knowledge in base_knowledge:
            self.engine.learn(
                content=knowledge,
                source="bot_initialization",
                tags=["bot_info", "base_knowledge"],
                importance=0.9
            )
    
    def start_conversation(self, user_id, user_name=None):
        """–ù–∞—á–∞–ª–æ –Ω–æ–≤–æ–π –±–µ—Å–µ–¥—ã —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º"""
        conversation_id = str(uuid.uuid4())
        
        self.conversations[user_id] = {
            "id": conversation_id,
            "user_name": user_name or f"User_{user_id}",
            "started_at": datetime.now(),
            "messages": [],
            "context": {
                "user_id": user_id,
                "conversation_id": conversation_id,
                "user_preferences": {},
                "current_topic": None
            }
        }
        
        welcome_message = f"–ü—Ä–∏–≤–µ—Ç! –Ø {self.bot_name}. –ö–∞–∫ –¥–µ–ª–∞? –û —á–µ–º —Ö–æ—Ç–∏—Ç–µ –ø–æ–≥–æ–≤–æ—Ä–∏—Ç—å?"
        
        self._add_message(user_id, "bot", welcome_message)
        return welcome_message
    
    def send_message(self, user_id, message):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        
        if user_id not in self.conversations:
            self.start_conversation(user_id)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –∏—Å—Ç–æ—Ä–∏—é
        self._add_message(user_id, "user", message)
        
        # –û–±—É—á–∞–µ–º —Å–∏—Å—Ç–µ–º—É –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        self._learn_from_user_message(user_id, message)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = self._generate_response(user_id, message)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –±–æ—Ç–∞ –≤ –∏—Å—Ç–æ—Ä–∏—é
        self._add_message(user_id, "bot", response["text"])
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        self._update_context(user_id, message, response)
        
        return response
    
    def _add_message(self, user_id, sender, text):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –∏—Å—Ç–æ—Ä–∏—é –±–µ—Å–µ–¥—ã"""
        message = {
            "sender": sender,
            "text": text,
            "timestamp": datetime.now(),
            "message_id": str(uuid.uuid4())
        }
        
        self.conversations[user_id]["messages"].append(message)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ 50 —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
        if len(self.conversations[user_id]["messages"]) > 50:
            self.conversations[user_id]["messages"] = \
                self.conversations[user_id]["messages"][-50:]
    
    def _learn_from_user_message(self, user_id, message):
        """–û–±—É—á–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        conversation = self.conversations[user_id]
        user_name = conversation["user_name"]
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞–Ω–∏–µ
        contextualized_message = f"{user_name} —Å–∫–∞–∑–∞–ª: {message}"
        
        self.engine.learn(
            content=contextualized_message,
            source=f"conversation:{conversation['id']}",
            tags=["conversation", "user_input", user_id],
            importance=0.7
        )
    
    def _generate_response(self, user_id, message):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        conversation = self.conversations[user_id]
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        context = self._build_query_context(user_id)
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å –∫ —Å–∏—Å—Ç–µ–º–µ
        response = self.engine.query(
            question=message,
            context=context,
            include_explanations=True,
            max_results=3
        )
        
        # –ï—Å–ª–∏ —Å–∏—Å—Ç–µ–º–Ω—ã–π –æ—Ç–≤–µ—Ç —Å–ª–∏—à–∫–æ–º —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π, –¥–µ–ª–∞–µ–º –µ–≥–æ –±–æ–ª–µ–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–º
        bot_response = self._make_conversational(response.primary_response, conversation)
        
        return {
            "text": bot_response,
            "confidence": response.confidence,
            "context_used": len(context),
            "related_topics": response.related_concepts[:3],
            "processing_time": response.processing_time
        }
    
    def _build_query_context(self, user_id):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        conversation = self.conversations[user_id]
        
        # –ë–∞–∑–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = {
            "user_id": user_id,
            "user_name": conversation["user_name"],
            "conversation_id": conversation["id"],
            "bot_name": self.bot_name
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        recent_messages = conversation["messages"][-10:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Å–æ–æ–±—â–µ–Ω–∏–π
        if recent_messages:
            context["recent_conversation"] = [
                f"{msg['sender']}: {msg['text']}" 
                for msg in recent_messages
            ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é —Ç–µ–º—É, –µ—Å–ª–∏ –µ—Å—Ç—å
        if conversation["context"]["current_topic"]:
            context["current_topic"] = conversation["context"]["current_topic"]
        
        return context
    
    def _make_conversational(self, system_response, conversation):
        """–î–µ–ª–∞–µ–º –æ—Ç–≤–µ—Ç –±–æ–ª–µ–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–º"""
        user_name = conversation["user_name"]
        
        # –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–æ—Å—Ç–∏
        if len(system_response) > 200:
            # –î–ª–∏–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç - –¥–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–±–∏–≤–∫—É
            return f"{system_response}\n\n–ß—Ç–æ –µ—â–µ –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç –ø–æ —ç—Ç–æ–π —Ç–µ–º–µ?"
        elif system_response.lower().startswith("—è –Ω–µ –∑–Ω–∞—é"):
            # –ù–µ–∑–Ω–∞–Ω–∏–µ - –±–æ–ª–µ–µ –¥—Ä—É–∂–µ–ª—é–±–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞
            return f"–•–º, —è –ø–æ–∫–∞ –Ω–µ —É–≤–µ—Ä–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å. –ú–æ–∂–µ—Ç, —Ä–∞—Å—Å–∫–∞–∂–µ—Ç–µ –º–Ω–µ –±–æ–ª—å—à–µ –æ–± —ç—Ç–æ–º?"
        else:
            # –û–±—ã—á–Ω—ã–π –æ—Ç–≤–µ—Ç
            return system_response
    
    def _update_context(self, user_id, user_message, bot_response):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –±–µ—Å–µ–¥—ã"""
        conversation = self.conversations[user_id]
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º—É —Å–æ–æ–±—â–µ–Ω–∏—è —á–µ—Ä–µ–∑ NLP
        nlp = self.engine.get_component('nlp')
        nlp_result = nlp.process_text(user_message, extract_entities=True)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∫–∞–∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ–º—ã
        if hasattr(nlp_result, 'entities') and nlp_result.entities:
            entities = [entity.text for entity in nlp_result.entities[:3]]
            if entities:
                conversation["context"]["current_topic"] = entities[0]
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–º—ã
        current_topic = conversation["context"]["current_topic"]
        if current_topic:
            preferences = conversation["context"]["user_preferences"]
            preferences[current_topic] = preferences.get(current_topic, 0) + 1
    
    def get_conversation_summary(self, user_id):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—é–º–µ –±–µ—Å–µ–¥—ã"""
        if user_id not in self.conversations:
            return {"error": "–ë–µ—Å–µ–¥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"}
        
        conversation = self.conversations[user_id]
        messages = conversation["messages"]
        
        if not messages:
            return {"summary": "–ë–µ—Å–µ–¥–∞ —Ç–æ–ª—å–∫–æ –Ω–∞—á–∞–ª–∞—Å—å"}
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–µ—Å–µ–¥—ã
        total_messages = len(messages)
        user_messages = len([m for m in messages if m["sender"] == "user"])
        bot_messages = len([m for m in messages if m["sender"] == "bot"])
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã
        topics = list(conversation["context"]["user_preferences"].keys())
        duration = datetime.now() - conversation["started_at"]
        
        return {
            "conversation_id": conversation["id"],
            "user_name": conversation["user_name"],
            "duration_minutes": int(duration.total_seconds() / 60),
            "total_messages": total_messages,
            "user_messages": user_messages,
            "bot_messages": bot_messages,
            "main_topics": topics[:5],
            "current_topic": conversation["context"]["current_topic"],
            "started_at": conversation["started_at"].isoformat()
        }
    
    def get_user_profile(self, user_id):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ –±–µ—Å–µ–¥"""
        if user_id not in self.conversations:
            return {"error": "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω"}
        
        conversation = self.conversations[user_id]
        
        # –ê–Ω–∞–ª–∏–∑ –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        preferences = conversation["context"]["user_preferences"]
        sorted_interests = sorted(preferences.items(), key=lambda x: x[1], reverse=True)
        
        # –°—Ç–∏–ª—å –æ–±—â–µ–Ω–∏—è (–∞–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã —Å–æ–æ–±—â–µ–Ω–∏–π)
        user_messages = [m for m in conversation["messages"] if m["sender"] == "user"]
        if user_messages:
            avg_message_length = sum(len(m["text"]) for m in user_messages) / len(user_messages)
            communication_style = "–∫—Ä–∞—Ç–∫–∏–π" if avg_message_length < 50 else "–ø–æ–¥—Ä–æ–±–Ω—ã–π"
        else:
            communication_style = "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π"
        
        return {
            "user_name": conversation["user_name"],
            "primary_interests": [interest for interest, count in sorted_interests[:5]],
            "communication_style": communication_style,
            "total_interactions": len(user_messages),
            "favorite_topics": dict(sorted_interests[:3]),
            "profile_confidence": min(len(user_messages) / 10, 1.0)  # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –ø—Ä–æ—Ñ–∏–ª–µ
        }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —á–∞—Ç-–±–æ—Ç–∞
bot = ContextualChatBot("–ù–µ–π—Ä–æ–ë–æ—Ç")

# –ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ—Å–µ–¥—ã
user_id = "user_123"

# –ù–∞—á–∞–ª–æ –±–µ—Å–µ–¥—ã
welcome = bot.start_conversation(user_id, "–ê–Ω–Ω–∞")
print(f"–ë–æ—Ç: {welcome}")

# –î–∏–∞–ª–æ–≥
messages = [
    "–ü—Ä–∏–≤–µ—Ç! –ú–µ–Ω—è –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
    "–ß—Ç–æ —Ç–∞–∫–æ–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏?",
    "–ê –∫–∞–∫ –æ–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏?",
    "–°–ø–∞—Å–∏–±–æ –∑–∞ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è! –ê —á—Ç–æ —Ç—ã –∑–Ω–∞–µ—à—å –æ Python?",
    "–ö–∞–∫–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è ML?"
]

for message in messages:
    print(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {message}")
    response = bot.send_message(user_id, message)
    print(f"–ë–æ—Ç: {response['text']}")
    print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {response['confidence']:.2f}")
    if response['related_topics']:
        print(f"–°–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã: {', '.join(response['related_topics'])}")
    print("-" * 50)

# –ê–Ω–∞–ª–∏–∑ –±–µ—Å–µ–¥—ã
summary = bot.get_conversation_summary(user_id)
print("\n–†–µ–∑—é–º–µ –±–µ—Å–µ–¥—ã:")
print(json.dumps(summary, indent=2, ensure_ascii=False))

profile = bot.get_user_profile(user_id)
print("\n–ü—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:")
print(json.dumps(profile, indent=2, ensure_ascii=False))
```

---

# ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

## –¢–∏–ø—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π

### Default Configuration
```json
{
  "engine_name": "default_neurograph",
  "components": {
    "semgraph": {
      "type": "memory_efficient",
      "params": {}
    },
    "contextvec": {
      "type": "dynamic",
      "params": {
        "vector_size": 384,
        "use_indexing": true
      }
    },
    "memory": {
      "params": {
        "stm_capacity": 100,
        "ltm_capacity": 10000,
        "use_semantic_indexing": true
      }
    },
    "nlp": {
      "params": {
        "language": "ru",
        "use_spacy": true
      }
    },
    "processor": {
      "params": {
        "confidence_threshold": 0.5,
        "max_depth": 5
      }
    },
    "propagation": {
      "params": {
        "max_iterations": 100,
        "activation_threshold": 0.1
      }
    }
  },
  "performance": {
    "max_concurrent_requests": 10,
    "default_timeout": 30.0,
    "enable_caching": true,
    "cache_ttl": 300
  }
}
```

### Lightweight Configuration
```json
{
  "engine_name": "lightweight_neurograph",
  "components": {
    "memory": {
      "params": {
        "stm_capacity": 25,
        "ltm_capacity": 500,
        "use_semantic_indexing": false
      }
    },
    "nlp": {
      "params": {
        "language": "ru",
        "use_spacy": false
      }
    },
    "processor": {
      "params": {
        "confidence_threshold": 0.3,
        "max_depth": 2
      }
    }
  },
  "performance": {
    "max_concurrent_requests": 3,
    "default_timeout": 10.0,
    "enable_caching": false
  }
}
```

### Production Configuration
```json
{
  "engine_name": "production_neurograph",
  "components": {
    "semgraph": {
      "type": "persistent",
      "params": {
        "file_path": "/data/knowledge_graph.json",
        "auto_save_interval": 300.0
      }
    },
    "memory": {
      "params": {
        "stm_capacity": 200,
        "ltm_capacity": 50000,
        "auto_consolidation": true,
        "consolidation_interval": 600.0
      }
    }
  },
  "performance": {
    "max_concurrent_requests": 100,
    "default_timeout": 60.0,
    "enable_caching": true,
    "cache_ttl": 900
  },
  "monitoring": {
    "enable_metrics": true,
    "enable_health_checks": true,
    "metrics_interval": 60
  },
  "logging": {
    "level": "INFO",
    "file": "/logs/neurograph.log",
    "rotation": "100MB",
    "retention": "30 days"
  }
}
```

## –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

```python
from neurograph.integration import IntegrationConfig

# –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ
config = IntegrationConfig(
    engine_name="my_custom_engine",
    components={
        "memory": {
            "params": {
                "stm_capacity": 150,
                "ltm_capacity": 20000
            }
        },
        "nlp": {
            "params": {
                "language": "en",
                "confidence_threshold": 0.8
            }
        }
    },
    performance={
        "max_concurrent_requests": 50,
        "enable_caching": True,
        "cache_ttl": 600
    }
)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
config.save_to_file("my_config.json")

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–≤–∏–∂–∫–∞ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
from neurograph.integration import NeuroGraphEngine
engine = NeuroGraphEngine(config)
```

## –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è

```bash
# –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
export NEUROGRAPH_CONFIG_FILE="/path/to/config.json"
export NEUROGRAPH_LOG_LEVEL="INFO"
export NEUROGRAPH_LOG_FILE="/logs/neurograph.log"

# –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
export NEUROGRAPH_DATA_DIR="/data"
export NEUROGRAPH_GRAPH_FILE="/data/graph.json"
export NEUROGRAPH_MEMORY_DIR="/data/memory"

# –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
export NEUROGRAPH_MAX_WORKERS="4"
export NEUROGRAPH_CACHE_TTL="300"
export NEUROGRAPH_REQUEST_TIMEOUT="30"

# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
export NEUROGRAPH_SPACY_MODEL="ru_core_news_sm"
export NEUROGRAPH_VECTOR_SIZE="384"
```

---

# üîå –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

## Web Framework Integration

### Flask Integration
```python
from flask import Flask, request, jsonify
from neurograph.integration import create_default_engine
import logging

app = Flask(__name__)
engine = create_default_engine()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@app.route('/api/learn', methods=['POST'])
def learn():
    try:
        data = request.get_json()
        content = data.get('content', '')
        source = data.get('source', 'api')
        tags = data.get('tags', [])
        
        if not content:
            return jsonify({'error': 'Content is required'}), 400
        
        response = engine.learn(
            content=content,
            source=source,
            tags=tags,
            importance=data.get('importance', 0.7)
        )
        
        return jsonify({
            'success': response.success,
            'message': response.primary_response,
            'processing_time': response.processing_time
        })
        
    except Exception as e:
        logger.error(f"Error in /api/learn: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/query', methods=['POST'])
def query():
    try:
        data = request.get_json()
        question = data.get('question', '')
        context = data.get('context', {})
        
        if not question:
            return jsonify({'error': 'Question is required'}), 400
        
        response = engine.query(
            question=question,
            context=context,
            include_explanations=data.get('include_explanations', True),
            max_results=data.get('max_results', 5)
        )
        
        return jsonify({
            'success': response.success,
            'answer': response.primary_response,
            'confidence': response.confidence,
            'sources': response.sources,
            'related_concepts': response.related_concepts,
            'processing_time': response.processing_time
        })
        
    except Exception as e:
        logger.error(f"Error in /api/query: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/health', methods=['GET'])
def health():
    try:
        health_status = engine.get_health_status()
        return jsonify(health_status)
    except Exception as e:
        return jsonify({'status': 'unhealthy', 'error': str(e)}), 500

@app.route('/api/stats', methods=['GET'])
def stats():
    try:
        stats = engine.get_system_statistics()
        return jsonify(stats)
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# Graceful shutdown
import atexit
def shutdown_engine():
    logger.info("Shutting down NeuroGraph engine...")
    engine.shutdown()

atexit.register(shutdown_engine)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=False)
```

### Django Integration
```python
# django_app/neurograph_service.py
from django.conf import settings
from neurograph.integration import create_default_engine
import logging

logger = logging.getLogger(__name__)

class NeuroGraphService:
    _instance = None
    _engine = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if self._engine is None:
            config_file = getattr(settings, 'NEUROGRAPH_CONFIG', None)
            if config_file:
                from neurograph.integration import NeuroGraphEngine, IntegrationConfig
                config = IntegrationConfig.load_from_file(config_file)
                self._engine = NeuroGraphEngine(config)
            else:
                self._engine = create_default_engine()
            
            logger.info("NeuroGraph engine initialized")
    
    @property
    def engine(self):
        return self._engine
    
    def shutdown(self):
        if self._engine:
            self._engine.shutdown()
            logger.info("NeuroGraph engine shutdown")

# Singleton instance
neurograph_service = NeuroGraphService()

# django_app/views.py
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from .neurograph_service import neurograph_service
import json

@csrf_exempt
@require_http_methods(["POST"])
def neurograph_learn(request):
    try:
        data = json.loads(request.body)
        response = neurograph_service.engine.learn(
            content=data.get('content', ''),
            source=data.get('source', 'django_api'),
            tags=data.get('tags', [])
        )
        
        return JsonResponse({
            'success': response.success,
            'message': response.primary_response
        })
        
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)

@csrf_exempt
@require_http_methods(["POST"])
def neurograph_query(request):
    try:
        data = json.loads(request.body)
        response = neurograph_service.engine.query(
            question=data.get('question', ''),
            context=data.get('context', {})
        )
        
        return JsonResponse({
            'success': response.success,
            'answer': response.primary_response,
            'confidence': response.confidence
        })
        
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)

# django_app/apps.py
from django.apps import AppConfig
from django.conf import settings

class NeuroGraphAppConfig(AppConfig):
    default_auto_field = 'django.db.models.BigAutoField'
    name = 'django_app'
    
    def ready(self):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NeuroGraph –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ Django
        from .neurograph_service import neurograph_service
        
        # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —Å–∏–≥–Ω–∞–ª–∞ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        import signal
        import sys
        
        def signal_handler(sig, frame):
            neurograph_service.shutdown()
            sys.exit(0)
        
        signal.signal(signal.SIGTERM, signal_handler)
        signal.signal(signal.SIGINT, signal_handler)
```

## Database Integration

### SQLAlchemy Integration
```python
from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Float
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from neurograph.integration import create_default_engine
from datetime import datetime
import json

Base = declarative_base()

class KnowledgeEntry(Base):
    __tablename__ = 'knowledge_entries'
    
    id = Column(Integer, primary_key=True)
    content = Column(Text, nullable=False)
    source = Column(String(255), default='unknown')
    tags = Column(Text)  # JSON string
    importance = Column(Float, default=0.5)
    processed = Column(Integer, default=0)  # Boolean as int
    created_at = Column(DateTime, default=datetime.utcnow)
    processed_at = Column(DateTime)

class QueryLog(Base):
    __tablename__ = 'query_log'
    
    id = Column(Integer, primary_key=True)
    question = Column(Text, nullable=False)
    answer = Column(Text)
    confidence = Column(Float)
    processing_time = Column(Float)
    created_at = Column(DateTime, default=datetime.utcnow)

class NeuroGraphDatabase:
    def __init__(self, database_url="sqlite:///neurograph.db"):
        self.engine_db = create_engine(database_url)
        Base.metadata.create_all(self.engine_db)
        
        Session = sessionmaker(bind=self.engine_db)
        self.session = Session()
        
        self.neurograph = create_default_engine()
    
    def add_knowledge(self, content, source="user", tags=None, importance=0.7):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏—è –≤ –ë–î –∏ –æ–±—É—á–µ–Ω–∏–µ NeuroGraph"""
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        entry = KnowledgeEntry(
            content=content,
            source=source,
            tags=json.dumps(tags or []),
            importance=importance
        )
        self.session.add(entry)
        self.session.commit()
        
        # –û–±—É—á–µ–Ω–∏–µ NeuroGraph
        response = self.neurograph.learn(
            content=content,
            source=source,
            tags=tags or [],
            importance=importance
        )
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if response.success:
            entry.processed = 1
            entry.processed_at = datetime.utcnow()
            self.session.commit()
        
        return entry.id, response.success
    
    def process_pending_knowledge(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π"""
        
        pending_entries = self.session.query(KnowledgeEntry).filter(
            KnowledgeEntry.processed == 0
        ).all()
        
        processed_count = 0
        for entry in pending_entries:
            try:
                tags = json.loads(entry.tags) if entry.tags else []
                
                response = self.neurograph.learn(
                    content=entry.content,
                    source=entry.source,
                    tags=tags,
                    importance=entry.importance
                )
                
                if response.success:
                    entry.processed = 1
                    entry.processed_at = datetime.utcnow()
                    processed_count += 1
                
            except Exception as e:
                print(f"Error processing entry {entry.id}: {e}")
        
        self.session.commit()
        return processed_count
    
    def query_with_logging(self, question, context=None):
        """–ó–∞–ø—Ä–æ—Å —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≤ –ë–î"""
        
        start_time = datetime.utcnow()
        
        response = self.neurograph.query(
            question=question,
            context=context or {},
            include_explanations=True
        )
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
        query_log = QueryLog(
            question=question,
            answer=response.primary_response,
            confidence=response.confidence,
            processing_time=response.processing_time
        )
        self.session.add(query_log)
        self.session.commit()
        
        return response
    
    def get_knowledge_stats(self):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–Ω–∞–Ω–∏–π –≤ –ë–î"""
        
        total_entries = self.session.query(KnowledgeEntry).count()
        processed_entries = self.session.query(KnowledgeEntry).filter(
            KnowledgeEntry.processed == 1
        ).count()
        
        recent_queries = self.session.query(QueryLog).filter(
            QueryLog.created_at >= datetime.utcnow().replace(hour=0, minute=0, second=0)
        ).count()
        
        return {
            "total_knowledge_entries": total_entries,
            "processed_entries": processed_entries,
            "pending_entries": total_entries - processed_entries,
            "queries_today": recent_queries
        }
    
    def cleanup_old_data(self, days=30):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        
        cutoff_date = datetime.utcnow() - timedelta(days=days)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤
        old_queries = self.session.query(QueryLog).filter(
            QueryLog.created_at < cutoff_date
        ).delete()
        
        self.session.commit()
        return old_queries

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
db = NeuroGraphDatabase("postgresql://user:pass@localhost/neurograph")

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π
entry_id, success = db.add_knowledge(
    "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞",
    source="textbook",
    tags=["AI", "ML", "education"],
    importance=0.9
)

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
processed = db.process_pending_knowledge()
print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {processed}")

# –ó–∞–ø—Ä–æ—Å —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
response = db.query_with_logging("–ß—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?")
print(f"–û—Ç–≤–µ—Ç: {response.primary_response}")

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
stats = db.get_knowledge_stats()
print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats}")
```

## Cloud Services Integration

### AWS Integration
```python
import boto3
from neurograph.integration import create_default_engine
import json
from datetime import datetime

class AWSNeuroGraphIntegration:
    def __init__(self, region='us-east-1'):
        self.region = region
        self.s3_client = boto3.client('s3', region_name=region)
        self.lambda_client = boto3.client('lambda', region_name=region)
        self.secrets_client = boto3.client('secretsmanager', region_name=region)
        
        self.neurograph = create_default_engine()
        
    def process_s3_documents(self, bucket_name, prefix="documents/"):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ S3"""
        
        response = self.s3_client.list_objects_v2(
            Bucket=bucket_name,
            Prefix=prefix
        )
        
        if 'Contents' not in response:
            return {"processed": 0, "errors": []}
        
        processed_count = 0
        errors = []
        
        for obj in response['Contents']:
            try:
                # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
                file_content = self.s3_client.get_object(
                    Bucket=bucket_name,
                    Key=obj['Key']
                )
                
                content = file_content['Body'].read().decode('utf-8')
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ NeuroGraph
                result = self.neurograph.learn(
                    content=content,
                    source=f"s3://{bucket_name}/{obj['Key']}",
                    tags=["s3_document", "auto_processed"]
                )
                
                if result.success:
                    processed_count += 1
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –æ–±—Ä–∞—Ç–Ω–æ –≤ S3
                    self._save_processing_result(bucket_name, obj['Key'], result)
                
            except Exception as e:
                errors.append(f"Error processing {obj['Key']}: {str(e)}")
        
        return {"processed": processed_count, "errors": errors}
    
    def _save_processing_result(self, bucket, key, result):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ S3"""
        
        result_data = {
            "processed_at": datetime.utcnow().isoformat(),
            "success": result.success,
            "confidence": result.confidence,
            "processing_time": result.processing_time,
            "structured_data": result.structured_data
        }
        
        result_key = key.replace(".txt", "_processed.json")
        
        self.s3_client.put_object(
            Bucket=bucket,
            Key=f"processed/{result_key}",
            Body=json.dumps(result_data, indent=2),
            ContentType='application/json'
        )
    
    def deploy_as_lambda(self, function_name="neurograph-processor"):
        """–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –∫–∞–∫ Lambda —Ñ—É–Ω–∫—Ü–∏—è"""
        
        # –ö–æ–¥ Lambda —Ñ—É–Ω–∫—Ü–∏–∏
        lambda_code = '''
import json
from neurograph.integration import create_lightweight_engine

engine = None

def lambda_handler(event, context):
    global engine
    
    if engine is None:
        engine = create_lightweight_engine()
    
    try:
        if event.get('action') == 'learn':
            response = engine.learn(
                content=event['content'],
                source=event.get('source', 'lambda'),
                tags=event.get('tags', [])
            )
        elif event.get('action') == 'query':
            response = engine.query(
                question=event['question'],
                context=event.get('context', {})
            )
        else:
            return {
                'statusCode': 400,
                'body': json.dumps({'error': 'Invalid action'})
            }
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'success': response.success,
                'result': response.primary_response,
                'confidence': response.confidence
            })
        }
        
    except Exception as e:
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }
        '''
        
        # –°–æ–∑–¥–∞–Ω–∏–µ Lambda —Ñ—É–Ω–∫—Ü–∏–∏ (–ø—Å–µ–≤–¥–æ–∫–æ–¥)
        print(f"Lambda —Ñ—É–Ω–∫—Ü–∏—è {function_name} –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—é")
        print("–ö–æ–¥ —Ñ—É–Ω–∫—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –¥–ª—è —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è")
        
        return lambda_code

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
aws_integration = AWSNeuroGraphIntegration(region='us-east-1')

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ S3
result = aws_integration.process_s3_documents('my-documents-bucket')
print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {result['processed']}")

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ Lambda —Ñ—É–Ω–∫—Ü–∏–∏
lambda_code = aws_integration.deploy_as_lambda()
```

---

# üîß –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã

## –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –º–æ–¥—É–ª–µ–π

### –ë–∞–∑–æ–≤—ã–π –º–æ–¥—É–ª—å
```python
from neurograph.core import Component, Configurable
from neurograph.core.logging import get_logger
from abc import ABC, abstractmethod

# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –º–æ–¥—É–ª—è
class ICustomProcessor(ABC):
    @abstractmethod
    def process_data(self, data: str) -> dict:
        pass
    
    @abstractmethod
    def get_capabilities(self) -> list:
        pass

# –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –º–æ–¥—É–ª—è
class SentimentAnalysisModule(Component, Configurable, ICustomProcessor):
    def __init__(self, component_id: str = "sentiment_analyzer"):
        super().__init__(component_id)
        self.logger = get_logger(self.__class__.__name__)
        self.config = {}
        
        # –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è
        self.positive_words = set([
            "—Ö–æ—Ä–æ—à–æ", "–æ—Ç–ª–∏—á–Ω–æ", "–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ", "–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ", 
            "–≤–µ–ª–∏–∫–æ–ª–µ–ø–Ω–æ", "–ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ", "—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ"
        ])
        self.negative_words = set([
            "–ø–ª–æ—Ö–æ", "—É–∂–∞—Å–Ω–æ", "–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ", "–∫–æ—à–º–∞—Ä–Ω–æ",
            "–Ω–µ–ø—Ä–∏—è—Ç–Ω–æ", "—Ä–∞–∑–¥—Ä–∞–∂–∞–µ—Ç", "—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ"
        ])
    
    def initialize(self) -> bool:
        self.logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π")
        return True
    
        def initialize(self) -> bool:
        self.logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π")
        return True
    
    def shutdown(self) -> bool:
        self.logger.info("–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –º–æ–¥—É–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π")
        return True
    
    def configure(self, config: dict) -> bool:
        self.config = config
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —Å–ª–æ–≤–∞—Ä–µ–π, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã
        if 'positive_words_file' in config:
            self._load_word_list(config['positive_words_file'], self.positive_words)
        
        if 'negative_words_file' in config:
            self._load_word_list(config['negative_words_file'], self.negative_words)
        
        return True
    
    def get_config(self) -> dict:
        return self.config.copy()
    
    def process_data(self, data: str) -> dict:
        """–ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞"""
        
        words = data.lower().split()
        
        positive_count = sum(1 for word in words if word in self.positive_words)
        negative_count = sum(1 for word in words if word in self.negative_words)
        total_words = len(words)
        
        if total_words == 0:
            sentiment_score = 0.0
        else:
            sentiment_score = (positive_count - negative_count) / total_words
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è
        if sentiment_score > 0.1:
            sentiment = "positive"
        elif sentiment_score < -0.1:
            sentiment = "negative"
        else:
            sentiment = "neutral"
        
        return {
            "sentiment": sentiment,
            "sentiment_score": sentiment_score,
            "positive_words_found": positive_count,
            "negative_words_found": negative_count,
            "confidence": min(abs(sentiment_score) * 2, 1.0)
        }
    
    def get_capabilities(self) -> list:
        return [
            "sentiment_analysis",
            "emotion_detection",
            "text_polarity"
        ]
    
    def _load_word_list(self, file_path: str, word_set: set):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–∏—Å–∫–∞ —Å–ª–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                words = [line.strip().lower() for line in f if line.strip()]
                word_set.update(words)
            self.logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(words)} —Å–ª–æ–≤ –∏–∑ {file_path}")
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ª–æ–≤ –∏–∑ {file_path}: {e}")

# –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –º–æ–¥—É–ª—è –≤ —Å–∏—Å—Ç–µ–º–µ
from neurograph.integration import ProcessorFactory

# –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —Ñ–∞–±—Ä–∏–∫–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥—É–ª—è
ProcessorFactory.register_processor("sentiment_analysis", SentimentAnalysisModule)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ —Å–æ—Å—Ç–∞–≤–µ NeuroGraph
from neurograph.integration import create_default_engine

engine = create_default_engine()

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –º–æ–¥—É–ª—è
sentiment_module = SentimentAnalysisModule()
sentiment_module.initialize()

# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –¥–≤–∏–∂–∫–æ–º (—á–µ—Ä–µ–∑ –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤)
provider = engine.provider
provider.register_component("sentiment_analyzer", sentiment_module)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–æ–¥—É–ª—è
result = sentiment_module.process_data("–≠—Ç–æ –æ—Ç–ª–∏—á–Ω—ã–π –¥–µ–Ω—å! –Ø –æ—á–µ–Ω—å —Ä–∞–¥!")
print(f"–ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {result['sentiment']}, –û—Ü–µ–Ω–∫–∞: {result['sentiment_score']:.2f}")
```

### –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–Ω–≤–µ–π–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏

```python
from neurograph.integration.pipelines import BasePipeline
from neurograph.integration.base import ProcessingRequest, ProcessingResponse
import time

class SentimentAnalysisPipeline(BasePipeline):
    """–ö–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π"""
    
    def __init__(self):
        super().__init__("sentiment_analysis")
        self.sentiment_module = None
    
    def process(self, request: ProcessingRequest, provider) -> ProcessingResponse:
        start_time = time.time()
        
        try:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –º–æ–¥—É–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π
            if self.sentiment_module is None:
                self.sentiment_module = provider.get_component('sentiment_analyzer')
            
            # –ë–∞–∑–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ NLP
            nlp = provider.get_component('nlp')
            nlp_result = nlp.process_text(request.content)
            
            # –ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è
            sentiment_result = self.sentiment_module.process_data(request.content)
            
            # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ø–∞–º—è—Ç—å—é
            memory = provider.get_component('memory')
            if sentiment_result['sentiment'] != 'neutral':
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç—ã —Å –≤—ã—Ä–∞–∂–µ–Ω–Ω—ã–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ–º
                from neurograph.memory.base import MemoryItem
                import numpy as np
                
                memory_item = MemoryItem(
                    content=request.content,
                    embedding=np.random.random(384),  # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ - —á–µ—Ä–µ–∑ —ç–Ω–∫–æ–¥–µ—Ä
                    content_type="sentiment_text",
                    metadata={
                        "sentiment": sentiment_result['sentiment'],
                        "sentiment_score": sentiment_result['sentiment_score'],
                        "confidence": sentiment_result['confidence']
                    }
                )
                memory.add(memory_item)
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
            processing_time = time.time() - start_time
            
            response_text = f"–ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {sentiment_result['sentiment']} (–æ—Ü–µ–Ω–∫–∞: {sentiment_result['sentiment_score']:.2f})"
            
            return ProcessingResponse(
                success=True,
                primary_response=response_text,
                confidence=sentiment_result['confidence'],
                processing_time=processing_time,
                structured_data={
                    'nlp_result': nlp_result.__dict__ if hasattr(nlp_result, '__dict__') else {},
                    'sentiment_analysis': sentiment_result
                },
                explanation=[
                    f"–ù–∞–π–¥–µ–Ω–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {sentiment_result['positive_words_found']}",
                    f"–ù–∞–π–¥–µ–Ω–æ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {sentiment_result['negative_words_found']}",
                    f"–ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞: {sentiment_result['sentiment_score']:.3f}"
                ]
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.logger.error(f"–û—à–∏–±–∫–∞ –≤ –∫–æ–Ω–≤–µ–π–µ—Ä–µ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π: {e}")
            
            return ProcessingResponse(
                success=False,
                primary_response="–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è",
                confidence=0.0,
                processing_time=processing_time,
                error_message=str(e)
            )

# –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∫–æ–Ω–≤–µ–π–µ—Ä–∞
def register_sentiment_pipeline(engine):
    """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π"""
    sentiment_pipeline = SentimentAnalysisPipeline()
    engine.pipelines['sentiment_analysis'] = sentiment_pipeline
    return engine

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
engine = create_default_engine()

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥—É–ª—è –∏ –∫–æ–Ω–≤–µ–π–µ—Ä–∞
sentiment_module = SentimentAnalysisModule()
sentiment_module.initialize()
engine.provider.register_component("sentiment_analyzer", sentiment_module)

register_sentiment_pipeline(engine)

# –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–Ω–≤–µ–π–µ—Ä
request = ProcessingRequest(
    content="–°–µ–≥–æ–¥–Ω—è –∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π –¥–µ–Ω—å! –Ø —á—É–≤—Å—Ç–≤—É—é —Å–µ–±—è –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ!",
    request_type="sentiment_analysis"
)

response = engine.process_request(request)
print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {response.primary_response}")
print(f"–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {response.structured_data['sentiment_analysis']}")
```

## –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∞–¥–∞–ø—Ç–µ—Ä–æ–≤

```python
from neurograph.integration.adapters import BaseAdapter
from typing import Any, Dict, List

class CustomDataAdapter(BaseAdapter):
    """–ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, data_source_config: Dict[str, Any]):
        super().__init__("custom_data_adapter")
        self.config = data_source_config
        self.supported_formats = ["json", "xml", "csv"]
    
    def adapt(self, source_data: Any, target_format: str) -> Dict[str, Any]:
        """–ê–¥–∞–ø—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –≤ —Ñ–æ—Ä–º–∞—Ç NeuroGraph"""
        
        if target_format == "neurograph_entities":
            return self._convert_to_entities(source_data)
        elif target_format == "neurograph_relations":
            return self._convert_to_relations(source_data)
        elif target_format == "memory_items":
            return self._convert_to_memory_items(source_data)
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {target_format}")
    
    def _convert_to_entities(self, data: Dict) -> List[Dict]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç —Å—É—â–Ω–æ—Å—Ç–µ–π NeuroGraph"""
        entities = []
        
        if isinstance(data, dict):
            for key, value in data.items():
                if isinstance(value, str) and len(value) > 2:
                    entities.append({
                        "text": value,
                        "entity_type": self._classify_entity_type(key, value),
                        "confidence": 0.8,
                        "source": "custom_adapter",
                        "metadata": {"original_key": key}
                    })
        
        return entities
    
    def _convert_to_relations(self, data: Dict) -> List[Dict]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç –æ—Ç–Ω–æ—à–µ–Ω–∏–π NeuroGraph"""
        relations = []
        
        if isinstance(data, dict):
            for key, value in data.items():
                if isinstance(value, dict):
                    for sub_key, sub_value in value.items():
                        relations.append({
                            "subject": key,
                            "predicate": self._normalize_predicate(sub_key),
                            "object": str(sub_value),
                            "confidence": 0.7,
                            "source": "custom_adapter"
                        })
                elif isinstance(value, list):
                    for item in value:
                        relations.append({
                            "subject": key,
                            "predicate": "contains",
                            "object": str(item),
                            "confidence": 0.6,
                            "source": "custom_adapter"
                        })
        
        return relations
    
    def _convert_to_memory_items(self, data: Any) -> List[Dict]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–∞–º—è—Ç–∏"""
        items = []
        
        if isinstance(data, dict):
            for key, value in data.items():
                content = f"{key}: {value}"
                items.append({
                    "content": content,
                    "content_type": "custom_data",
                    "metadata": {
                        "source": "custom_adapter",
                        "original_key": key,
                        "data_type": type(value).__name__
                    }
                })
        
        return items
    
    def _classify_entity_type(self, key: str, value: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ —Å—É—â–Ω–æ—Å—Ç–∏"""
        key_lower = key.lower()
        
        if any(word in key_lower for word in ['name', 'title', 'label']):
            return "ENTITY_NAME"
        elif any(word in key_lower for word in ['date', 'time', 'when']):
            return "DATE"
        elif any(word in key_lower for word in ['place', 'location', 'where']):
            return "LOCATION"
        elif any(word in key_lower for word in ['person', 'author', 'user']):
            return "PERSON"
        else:
            return "CONCEPT"
    
    def _normalize_predicate(self, key: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–∏–∫–∞—Ç–∞"""
        key_lower = key.lower()
        
        predicate_mapping = {
            'has': 'has',
            'is': 'is_a',
            'contains': 'contains',
            'belongs': 'belongs_to',
            'located': 'located_in',
            'created': 'created_by',
            'owns': 'owns'
        }
        
        for pattern, predicate in predicate_mapping.items():
            if pattern in key_lower:
                return predicate
        
        return key_lower.replace(' ', '_')

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–∞
custom_adapter = CustomDataAdapter({
    "source_type": "json",
    "encoding": "utf-8"
})

# –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
sample_data = {
    "product_name": "–°–º–∞—Ä—Ç—Ñ–æ–Ω iPhone 15",
    "manufacturer": "Apple",
    "release_date": "2023-09-15",
    "features": ["Face ID", "Wireless Charging", "5G"],
    "specifications": {
        "screen_size": "6.1 inch",
        "storage": "128GB",
        "ram": "6GB"
    }
}

# –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
entities = custom_adapter.adapt(sample_data, "neurograph_entities")
relations = custom_adapter.adapt(sample_data, "neurograph_relations")
memory_items = custom_adapter.adapt(sample_data, "memory_items")

print("–°—É—â–Ω–æ—Å—Ç–∏:")
for entity in entities:
    print(f"  - {entity['text']} ({entity['entity_type']})")

print("\n–û—Ç–Ω–æ—à–µ–Ω–∏—è:")
for relation in relations:
    print(f"  - {relation['subject']} {relation['predicate']} {relation['object']}")

print("\n–≠–ª–µ–º–µ–Ω—Ç—ã –ø–∞–º—è—Ç–∏:")
for item in memory_items:
    print(f"  - {item['content']}")
```

---

# üöÄ –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ

## Docker Deployment

### –ë–∞–∑–æ–≤—ã–π Dockerfile
```dockerfile
FROM python:3.11-slim

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
RUN useradd --create-home --shell /bin/bash neurograph
USER neurograph
WORKDIR /home/neurograph

# –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
COPY --chown=neurograph:neurograph requirements.txt .
RUN pip install --user --no-cache-dir -r requirements.txt

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ NeuroGraph
COPY --chown=neurograph:neurograph . .
RUN pip install --user -e .

# –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
RUN mkdir -p data logs config

# –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
ENV PYTHONPATH=/home/neurograph
ENV NEUROGRAPH_DATA_DIR=/home/neurograph/data
ENV NEUROGRAPH_LOG_DIR=/home/neurograph/logs
ENV NEUROGRAPH_CONFIG_DIR=/home/neurograph/config

# –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
COPY --chown=neurograph:neurograph config/production.json config/

# –ü–æ—Ä—Ç –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
EXPOSE 8080

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "from app import health_check; health_check()" || exit 1

# –ö–æ–º–∞–Ω–¥–∞ –∑–∞–ø—É—Å–∫–∞
CMD ["python", "app.py"]
```

### Docker Compose –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞
```yaml
version: '3.8'

services:
  neurograph-app:
    build: .
    container_name: neurograph-app
    restart: unless-stopped
    ports:
      - "8080:8080"
    environment:
      - NEUROGRAPH_ENV=production
      - POSTGRES_HOST=postgres
      - REDIS_HOST=redis
    volumes:
      - ./data:/home/neurograph/data
      - ./logs:/home/neurograph/logs
      - ./config:/home/neurograph/config
    depends_on:
      - postgres
      - redis
    networks:
      - neurograph-network
    
  postgres:
    image: postgres:15
    container_name: neurograph-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: neurograph
      POSTGRES_USER: neurograph
      POSTGRES_PASSWORD: secure_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - neurograph-network
    
  redis:
    image: redis:7-alpine
    container_name: neurograph-redis
    restart: unless-stopped
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    networks:
      - neurograph-network
    
  nginx:
    image: nginx:alpine
    container_name: neurograph-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - neurograph-app
    networks:
      - neurograph-network

volumes:
  postgres_data:
  redis_data:

networks:
  neurograph-network:
    driver: bridge
```

### Nginx –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
```nginx
# nginx.conf
events {
    worker_connections 1024;
}

http {
    upstream neurograph_backend {
        server neurograph-app:8080;
    }
    
    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    
    server {
        listen 80;
        server_name your-domain.com;
        
        # Redirect HTTP to HTTPS
        return 301 https://$server_name$request_uri;
    }
    
    server {
        listen 443 ssl http2;
        server_name your-domain.com;
        
        # SSL configuration
        ssl_certificate /etc/nginx/ssl/cert.pem;
        ssl_certificate_key /etc/nginx/ssl/key.pem;
        ssl_protocols TLSv1.2 TLSv1.3;
        
        # Security headers
        add_header X-Frame-Options DENY;
        add_header X-Content-Type-Options nosniff;
        add_header X-XSS-Protection "1; mode=block";
        
        # API endpoints
        location /api/ {
            limit_req zone=api burst=20 nodelay;
            
            proxy_pass http://neurograph_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Timeouts
            proxy_connect_timeout 30s;
            proxy_send_timeout 30s;
            proxy_read_timeout 60s;
        }
        
        # Health check
        location /health {
            proxy_pass http://neurograph_backend;
            access_log off;
        }
        
        # Static files (–µ—Å–ª–∏ –µ—Å—Ç—å)
        location /static/ {
            alias /var/www/static/;
            expires 30d;
            add_header Cache-Control "public, immutable";
        }
    }
}
```

## Kubernetes Deployment

### Deployment –º–∞–Ω–∏—Ñ–µ—Å—Ç
```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: neurograph-app
  labels:
    app: neurograph
spec:
  replicas: 3
  selector:
    matchLabels:
      app: neurograph
  template:
    metadata:
      labels:
        app: neurograph
    spec:
      containers:
      - name: neurograph
        image: neurograph/neurograph:latest
        ports:
        - containerPort: 8080
        env:
        - name: NEUROGRAPH_ENV
          value: "production"
        - name: POSTGRES_HOST
          value: "postgres-service"
        - name: REDIS_HOST
          value: "redis-service"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
        volumeMounts:
        - name: config-volume
          mountPath: /home/neurograph/config
        - name: data-volume
          mountPath: /home/neurograph/data
      volumes:
      - name: config-volume
        configMap:
          name: neurograph-config
      - name: data-volume
        persistentVolumeClaim:
          claimName: neurograph-data-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: neurograph-service
spec:
  selector:
    app: neurograph
  ports:
  - port: 80
    targetPort: 8080
  type: ClusterIP
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: neurograph-config
data:
  production.json: |
    {
      "engine_name": "k8s_neurograph",
      "components": {
        "memory": {
          "params": {
            "stm_capacity": 200,
            "ltm_capacity": 50000
          }
        }
      },
      "performance": {
        "max_concurrent_requests": 50,
        "enable_caching": true
      }
    }
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: neurograph-data-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
```

### HorizontalPodAutoscaler
```yaml
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: neurograph-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: neurograph-app
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

---

# üîç Troubleshooting

## –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ—à–µ–Ω–∏—è

### –ü—Ä–æ–±–ª–µ–º—ã —Å —É—Å—Ç–∞–Ω–æ–≤–∫–æ–π

#### –û—à–∏–±–∫–∞: "No module named 'neurograph'"
```bash
# –†–µ—à–µ–Ω–∏–µ 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤ development —Ä–µ–∂–∏–º–µ
pip install -e .

# –†–µ—à–µ–Ω–∏–µ 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:/path/to/neurograph"

# –†–µ—à–µ–Ω–∏–µ 3: –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∫–∞
pip uninstall neurograph
pip install neurograph
```

#### –û—à–∏–±–∫–∞: "Unable to build wheels"
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ build tools
pip install --upgrade pip setuptools wheel

# –î–ª—è Ubuntu/Debian
sudo apt-get install build-essential python3-dev

# –î–ª—è CentOS/RHEL
sudo yum install gcc python3-devel

# –î–ª—è macOS
xcode-select --install
```

### –ü—Ä–æ–±–ª–µ–º—ã —Å –ø–∞–º—è—Ç—å—é

#### OutOfMemoryError –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –±–æ–ª—å—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
```python
# –†–µ—à–µ–Ω–∏–µ: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ lightweight –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
from neurograph.integration import create_lightweight_engine

engine = create_lightweight_engine()

# –ò–ª–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–∞–º—è—Ç–∏
from neurograph.integration import IntegrationConfig

config = IntegrationConfig(
    components={
        "memory": {
            "params": {
                "stm_capacity": 50,    # –£–º–µ–Ω—å—à–µ–Ω–æ
                "ltm_capacity": 1000   # –£–º–µ–Ω—å—à–µ–Ω–æ
            }
        }
    }
)
```

#### –ú–µ–¥–ª–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å–∏—Å—Ç–µ–º—ã
```python
# –†–µ—à–µ–Ω–∏–µ: –í–∫–ª—é—á–µ–Ω–∏–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
config = IntegrationConfig(
    performance={
        "enable_caching": True,
        "cache_ttl": 600,
        "max_concurrent_requests": 5  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏
    },
    components={
        "nlp": {
            "params": {
                "use_spacy": False  # –û—Ç–∫–ª—é—á–µ–Ω–∏–µ spaCy –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ä–µ—Å—É—Ä—Å–æ–≤
            }
        }
    }
)
```

### –ü—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏

#### –û—à–∏–±–∫–∞: "Component not found"
```python
# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
engine = create_default_engine()
available_components = engine.provider.get_available_components()
print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:", available_components)

# –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
try:
    nlp = engine.get_component('nlp')
except Exception as e:
    print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è NLP: {e}")
    # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
    engine.provider.initialize_component('nlp')
```

#### –û—à–∏–±–∫–∞: "Failed to initialize spaCy model"
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ spaCy
python -m spacy download ru_core_news_sm

# –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ fallback –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
```

```python
config = IntegrationConfig(
    components={
        "nlp": {
            "params": {
                "use_spacy": False,  # –û—Ç–∫–ª—é—á–∏—Ç—å spaCy
                "fallback_to_rules": True
            }
        }
    }
)
```

### –ü—Ä–æ–±–ª–µ–º—ã —Å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é

#### –î–æ–ª–≥–æ–µ –≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞
```python
# –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
import time

start_time = time.time()
response = engine.query("test question")
end_time = time.time()

print(f"–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {end_time - start_time:.2f} —Å–µ–∫")
print(f"–°–∏—Å—Ç–µ–º–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {response.structured_data}")

# –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
components_time = {}
for component_name in ['nlp', 'memory', 'semgraph']:
    start = time.time()
    component = engine.get_component(component_name)
    # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
    components_time[component_name] = time.time() - start

print("–í—Ä–µ–º—è –ø–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º:", components_time)
```

#### –í—ã—Å–æ–∫–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ CPU
```python
# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤
from neurograph.core.resources import get_resource_usage

usage = get_resource_usage()
print(f"CPU: {usage['cpu_percent']}%")
print(f"–ü–∞–º—è—Ç—å: {usage['memory_rss'] / 1024 / 1024:.1f} MB")

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
config = IntegrationConfig(
    components={
        "propagation": {
            "params": {
                "max_iterations": 50,  # –£–º–µ–Ω—å—à–µ–Ω–æ
                "activation_threshold": 0.3  # –£–≤–µ–ª–∏—á–µ–Ω–æ
            }
        }
    }
)
```

### –ü—Ä–æ–±–ª–µ–º—ã —Å –¥–∞–Ω–Ω—ã–º–∏

#### –û—à–∏–±–∫–∞: "Failed to save graph data"
```python
# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤ –¥–æ—Å—Ç—É–ø–∞
import os

data_dir = "/path/to/data"
if not os.access(data_dir, os.W_OK):
    print(f"–ù–µ—Ç –ø—Ä–∞–≤ –∑–∞–ø–∏—Å–∏ –≤ {data_dir}")

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
os.makedirs(data_dir, exist_ok=True)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
import tempfile
temp_dir = tempfile.mkdtemp()
config = IntegrationConfig(
    components={
        "semgraph": {
            "type": "persistent",
            "params": {
                "file_path": f"{temp_dir}/graph.json"
            }
        }
    }
)
```

#### –ü–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≥—Ä–∞—Ñ–∞
```python
# –†–µ–∑–µ—Ä–≤–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
def backup_graph_data(engine, backup_path):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≥—Ä–∞—Ñ–∞"""
    graph = engine.get_component('semgraph')
    
    if hasattr(graph, 'save'):
        graph.save(backup_path)
        print(f"–†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {backup_path}")

def restore_graph_data(engine, backup_path):
    """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≥—Ä–∞—Ñ–∞ –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏"""
    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –≥—Ä–∞—Ñ–∞ –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏
        from neurograph.semgraph.impl.memory_graph import MemoryEfficientSemGraph
        restored_graph = MemoryEfficientSemGraph.load(backup_path)
        
        # –ó–∞–º–µ–Ω–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
        engine.provider.register_component('semgraph', restored_graph)
        print(f"–ì—Ä–∞—Ñ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–∑: {backup_path}")
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è: {e}")

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
backup_graph_data(engine, "backup_graph.json")
# –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏:
# restore_graph_data(engine, "backup_graph.json")
```

## –õ–æ–≥–∏ –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
```python
from neurograph.core.logging import setup_logging

# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
setup_logging(
    level="DEBUG",
    log_file="neurograph_debug.log",
    format="%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s"
)

# –í–∫–ª—é—á–µ–Ω–∏–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –º–æ–¥—É–ª–µ–π
import logging
logging.getLogger("neurograph.integration").setLevel(logging.DEBUG)
logging.getLogger("neurograph.memory").setLevel(logging.DEBUG)
logging.getLogger("neurograph.semgraph").setLevel(logging.DEBUG)
```

### –°–∏—Å—Ç–µ–º–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
```python
from neurograph.integration import ComponentMonitor, HealthChecker

# –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
monitor = ComponentMonitor(check_interval=10.0)
health_checker = HealthChecker()

# –ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
monitor.start_monitoring(engine.provider)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã
health = health_checker.get_overall_health()
print(f"–û–±—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ: {health['status']}")

if health['status'] != 'healthy':
    print("–ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:")
    for component, status in health['components'].items():
        if status['status'] != 'healthy':
            print(f"  - {component}: {status['error']}")

# –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
report = monitor.get_monitoring_report()
print(f"–ú–µ—Ç—Ä–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã:")
print(f"  - –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: {report['uptime_seconds']} —Å–µ–∫")
print(f"  - –í—Å–µ–≥–æ –∞–ª–µ—Ä—Ç–æ–≤: {len(report['recent_alerts'])}")
```

### –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –æ—Ç–ª–∞–¥–∫–∏

```python
class NeuroGraphDebugger:
    """–£—Ç–∏–ª–∏—Ç–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ NeuroGraph"""
    
    def __init__(self, engine):
        self.engine = engine
    
    def diagnose_system(self):
        """–ü–æ–ª–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã"""
        
        print("=== –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ NeuroGraph ===")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self._check_components()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self._check_configuration()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤
        self._check_resources()
        
        # –¢–µ—Å—Ç –±–∞–∑–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        self._test_basic_functionality()
    
    def _check_components(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        
        print("\n--- –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ ---")
        
        required_components = ['nlp', 'semgraph', 'memory', 'contextvec', 'processor']
        
        for component_name in required_components:
            try:
                component = self.engine.get_component(component_name)
                print(f"‚úÖ {component_name}: OK")
            except Exception as e:
                print(f"‚ùå {component_name}: –û–®–ò–ë–ö–ê - {e}")
    
    def _check_configuration(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        
        print("\n--- –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ---")
        
        try:
            config = self.engine.config
            print(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {config.engine_name}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫
            memory_config = config.components.get('memory', {}).get('params', {})
            stm_capacity = memory_config.get('stm_capacity', 0)
            ltm_capacity = memory_config.get('ltm_capacity', 0)
            
            if stm_capacity > 0 and ltm_capacity > 0:
                print(f"‚úÖ –ü–∞–º—è—Ç—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞: STM={stm_capacity}, LTM={ltm_capacity}")
            else:
                print("‚ö†Ô∏è  –ü–∞–º—è—Ç—å –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ –∏–ª–∏ –∏–º–µ–µ—Ç –Ω—É–ª–µ–≤—É—é –µ–º–∫–æ—Å—Ç—å")
            
        except Exception as e:
            print(f"‚ùå –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: –û–®–ò–ë–ö–ê - {e}")
    
    def _check_resources(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤"""
        
        print("\n--- –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ ---")
        
        try:
            from neurograph.core.resources import get_resource_usage
            usage = get_resource_usage()
            
            cpu_percent = usage.get('cpu_percent', 0)
            memory_mb = usage.get('memory_rss', 0) / 1024 / 1024
            
            print(f"CPU: {cpu_percent:.1f}%")
            print(f"–ü–∞–º—è—Ç—å: {memory_mb:.1f} MB")
            
            # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
            if cpu_percent > 80:
                print("‚ö†Ô∏è  –í—ã—Å–æ–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ CPU")
            if memory_mb > 1000:
                print("‚ö†Ô∏è  –í—ã—Å–æ–∫–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏")
            
        except Exception as e:
            print(f"‚ùå –†–µ—Å—É—Ä—Å—ã: –û–®–ò–ë–ö–ê - {e}")
    
    def _test_basic_functionality(self):
        """–¢–µ—Å—Ç –±–∞–∑–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
        
        print("\n--- –¢–µ—Å—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ ---")
        
        try:
            # –¢–µ—Å—Ç –æ–±—É—á–µ–Ω–∏—è
            learn_response = self.engine.learn("–¢–µ—Å—Ç: NeuroGraph —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ")
            if learn_response.success:
                print("‚úÖ –û–±—É—á–µ–Ω–∏–µ: OK")
            else:
                print(f"‚ùå –û–±—É—á–µ–Ω–∏–µ: –û–®–ò–ë–ö–ê - {learn_response.error_message}")
            
            # –¢–µ—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
            query_response = self.engine.query("–ß—Ç–æ —Ç–∞–∫–æ–µ —Ç–µ—Å—Ç?")
            if query_response.success:
                print("‚úÖ –ó–∞–ø—Ä–æ—Å—ã: OK")
                print(f"   –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {query_response.processing_time:.2f}—Å")
            else:
                print(f"‚ùå –ó–∞–ø—Ä–æ—Å—ã: –û–®–ò–ë–ö–ê - {query_response.error_message}")
            
        except Exception as e:
            print(f"‚ùå –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: –û–®–ò–ë–ö–ê - {e}")
    
    def benchmark_performance(self, num_operations=10):
        """–ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        
        print(f"\n=== –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ({num_operations} –æ–ø–µ—Ä–∞—Ü–∏–π) ===")
        
        import time
        
        # –ë–µ–Ω—á–º–∞—Ä–∫ –æ–±—É—á–µ–Ω–∏—è
        learn_times = []
        for i in range(num_operations):
            start = time.time()
            self.engine.learn(f"–¢–µ—Å—Ç–æ–≤–æ–µ –∑–Ω–∞–Ω–∏–µ –Ω–æ–º–µ—Ä {i}")
            learn_times.append(time.time() - start)
        
        # –ë–µ–Ω—á–º–∞—Ä–∫ –∑–∞–ø—Ä–æ—Å–æ–≤
        query_times = []
        for i in range(num_operations):
            start = time.time()
            self.engine.query(f"–í–æ–ø—Ä–æ—Å –Ω–æ–º–µ—Ä {i}")
            query_times.append(time.time() - start)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        avg_learn_time = sum(learn_times) / len(learn_times)
        avg_query_time = sum(query_times) / len(query_times)
        
        print(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {avg_learn_time:.3f}—Å")
        print(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∑–∞–ø—Ä–æ—Å–∞: {avg_query_time:.3f}—Å")
        print(f"–ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è: {1/avg_learn_time:.1f} –æ–ø/—Å")
        print(f"–ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–æ–≤: {1/avg_query_time:.1f} –æ–ø/—Å")

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ—Ç–ª–∞–¥—á–∏–∫–∞
debugger = NeuroGraphDebugger(engine)
debugger.diagnose_system()
debugger.benchmark_performance(num_operations=5)
```

## –ß–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ –≤–æ–ø—Ä–æ—Å—ã (FAQ)

### Q: –ö–∞–∫ —É–≤–µ–ª–∏—á–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã?

**A:** –ï—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–æ—Å–æ–±–æ–≤:

1. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:**
```python
config = IntegrationConfig(
    performance={
        "enable_caching": True,
        "cache_ttl": 900,  # –£–≤–µ–ª–∏—á–∏—Ç—å –≤—Ä–µ–º—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
        "max_concurrent_requests": 20
    },
    components={
        "memory": {
            "params": {
                "stm_capacity": 50,  # –£–º–µ–Ω—å—à–∏—Ç—å –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ä–µ—Å—É—Ä—Å–æ–≤
                "ltm_capacity": 5000
            }
        }
    }
)
```

2. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ lightweight –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏** –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –∑–∞–¥–∞—á

3. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ** –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —É–∑–∫–∏—Ö –º–µ—Å—Ç

### Q: –ö–∞–∫ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –º–µ–∂–¥—É –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–º–∏?

**A:** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ persistent –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é:

```python
config = IntegrationConfig(
    components={
        "semgraph": {
            "type": "persistent",
            "params": {
                "file_path": "./data/knowledge_graph.json",
                "auto_save_interval": 300.0
            }
        },
        "memory": {
            "params": {
                "use_persistent_storage": True,
                "storage_path": "./data/memory/"
            }
        }
    }
)
```

### Q: –ö–∞–∫ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏?

**A:** –°–æ–∑–¥–∞–π—Ç–µ –∞–¥–∞–ø—Ç–µ—Ä –¥–ª—è –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏:

```python
from neurograph.contextvec.adapters.base import BaseVectorAdapter

class CustomModelAdapter(BaseVectorAdapter):
    def __init__(self, model_path):
        self.model = self.load_custom_model(model_path)
    
    def encode(self, text):
        return self.model.encode(text)
    
    def encode_batch(self, texts):
        return self.model.encode_batch(texts)

# –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∞–¥–∞–ø—Ç–µ—Ä–∞
from neurograph.contextvec import ContextVectorsFactory
ContextVectorsFactory.register_adapter("custom_model", CustomModelAdapter)
```

### Q: –ö–∞–∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å —Å–∏—Å—Ç–µ–º—É –¥–ª—è –±–æ–ª—å—à–∏—Ö –Ω–∞–≥—Ä—É–∑–æ–∫?

**A:** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–¥—Ö–æ–¥–æ–≤:

1. **–ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ Kubernetes**
2. **–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –Ω–∞–≥—Ä—É–∑–∫–∏** –º–µ–∂–¥—É –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞–º–∏
3. **–ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã** (Redis/Memcached)
4. **–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞** —Ç—è–∂–µ–ª—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π

### Q: –ß—Ç–æ –¥–µ–ª–∞—Ç—å, –µ—Å–ª–∏ —Å–∏—Å—Ç–µ–º–∞ –ø–æ—Ç—Ä–µ–±–ª—è–µ—Ç —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏?

**A:** –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏:

```python
# 1. –£–º–µ–Ω—å—à–∏—Ç–µ —Ä–∞–∑–º–µ—Ä—ã –ø–∞–º—è—Ç–∏
config.components["memory"]["params"]["stm_capacity"] = 25
config.components["memory"]["params"]["ltm_capacity"] = 1000

# 2. –û—Ç–∫–ª—é—á–∏—Ç–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ
config.components["memory"]["params"]["use_semantic_indexing"] = False

# 3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–æ—Å—Ç—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã
config.components["nlp"]["params"]["use_spacy"] = False

# 4. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∑–∞–±—ã–≤–∞–Ω–∏–µ
config.components["memory"]["params"]["consolidation_interval"] = 60.0
```

---

# üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

## –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

- **–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è**: https://neurograph.readthedocs.io
- **GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π**: https://github.com/neurograph/neurograph
- **–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è**: https://github.com/neurograph/examples
- **–°–æ–æ–±—â–µ—Å—Ç–≤–æ**: https://discord.gg/neurograph
- **–ë–ª–æ–≥ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤**: https://blog.neurograph.ai

## –û–±—É—á–∞—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã

### –í–∏–¥–µ–æ—É—Ä–æ–∫–∏
1. "–í–≤–µ–¥–µ–Ω–∏–µ –≤ NeuroGraph" - –±–∞–∑–æ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
2. "–°–æ–∑–¥–∞–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞" - –ø–æ—à–∞–≥–æ–≤–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ
3. "–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è" - –ª—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏
4. "–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ" - DevOps –∞—Å–ø–µ–∫—Ç—ã

### –°—Ç–∞—Ç—å–∏ –∏ —Ç—É—Ç–æ—Ä–∏–∞–ª—ã
1. "–ù–µ–π—Ä–æ—Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏–µ —Å–∏—Å—Ç–µ–º—ã: —Ç–µ–æ—Ä–∏—è –∏ –ø—Ä–∞–∫—Ç–∏–∫–∞"
2. "–ë–∏–æ–º–æ—Ä—Ñ–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–∞–º—è—Ç–∏ –≤ NeuroGraph"
3. "–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏"
4. "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ò–ò-—Å–∏—Å—Ç–µ–º –≤ –æ–±–ª–∞–∫–µ"

## –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞

### –ö–∞–∫ –ø–æ–ª—É—á–∏—Ç—å –ø–æ–º–æ—â—å
1. **GitHub Issues** - –¥–ª—è –±–∞–≥–æ–≤ –∏ –∑–∞–ø—Ä–æ—Å–æ–≤ —Ñ—É–Ω–∫—Ü–∏–π
2. **Discord —Å–µ—Ä–≤–µ—Ä** - –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞
3. **Stack Overflow** - —Ç–µ–≥ `neurograph`
4. **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è** - –ø–æ–¥—Ä–æ–±–Ω—ã–µ –≥–∞–π–¥—ã –∏ API reference

### –ö–∞–∫ –≤–Ω–µ—Å—Ç–∏ –≤–∫–ª–∞–¥
1. **Fork** —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
2. **–°–æ–∑–¥–∞–π—Ç–µ –≤–µ—Ç–∫—É** –¥–ª—è –Ω–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
3. **–ù–∞–ø–∏—à–∏—Ç–µ —Ç–µ—Å—Ç—ã** –¥–ª—è –≤–∞—à–µ–≥–æ –∫–æ–¥–∞
4. **–û—Ç–ø—Ä–∞–≤—å—Ç–µ Pull Request** —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–π

---

# üéØ –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

NeuroGraph –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ—â–Ω—É—é –∏ –≥–∏–±–∫—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π. –≠—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã —Ä–∞–±–æ—Ç—ã —Å —Å–∏—Å—Ç–µ–º–æ–π:

## ‚úÖ –ß—Ç–æ –≤—ã –∏–∑—É—á–∏–ª–∏

- **–ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç** - —Å–æ–∑–¥–∞–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –∑–∞ –º–∏–Ω—É—Ç—ã
- **API Reference** - –ø–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –≤—Å–µ—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤
- **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã** - —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
- **–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è** - –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–¥ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∑–∞–¥–∞—á–∏
- **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è** - –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º —Å–∏—Å—Ç–µ–º–∞–º
- **–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ** - —Å–æ–∑–¥–∞–Ω–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–æ–¥—É–ª–µ–π
- **–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ** - –ø—Ä–æ–¥–∞–∫—à–Ω-–≥–æ—Ç–æ–≤—ã–µ —Ä–µ—à–µ–Ω–∏—è
- **Troubleshooting** - —Ä–µ—à–µ–Ω–∏–µ —Ç–∏–ø–∏—á–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º

## üöÄ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

1. **–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ** —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∫–æ–¥–∞
2. **–ò–∑—É—á–∏—Ç–µ** –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è
3. **–°–æ–∑–¥–∞–π—Ç–µ** —Å–≤–æ–µ –ø–µ—Ä–≤–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
4. **–ü–æ–¥–µ–ª–∏—Ç–µ—Å—å** –æ–ø—ã—Ç–æ–º —Å —Å–æ–æ–±—â–µ—Å—Ç–≤–æ–º
5. **–í–Ω–µ—Å–∏—Ç–µ –≤–∫–ª–∞–¥** –≤ —Ä–∞–∑–≤–∏—Ç–∏–µ –ø—Ä–æ–µ–∫—Ç–∞

## üí° –ü–æ–º–Ω–∏—Ç–µ

- NeuroGraph - —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞, –∞ —Ü–µ–ª–æ—Å—Ç–Ω–∞—è —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞
- –°–∏—Å—Ç–µ–º–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞ –Ω–∞ –ø—Ä–∏–Ω—Ü–∏–ø–∞—Ö –º–æ–¥—É–ª—å–Ω–æ—Å—Ç–∏ –∏ —Ä–∞—Å—à–∏—Ä—è–µ–º–æ—Å—Ç–∏
- –ë–∏–æ–º–æ—Ä—Ñ–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–µ–ª–∞–µ—Ç –ò–ò –±–æ–ª–µ–µ –ø–æ–Ω—è—Ç–Ω—ã–º –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–º
- –°–æ–æ–±—â–µ—Å—Ç–≤–æ –≤—Å–µ–≥–¥–∞ –≥–æ—Ç–æ–≤–æ –ø–æ–º–æ—á—å –≤ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á

**–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ –±—É–¥—É—â–µ–µ –Ω–µ–π—Ä–æ—Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–æ–≥–æ –ò–ò!** üß†‚ú®

---

*–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞ –¥–ª—è –≤–µ—Ä—Å–∏–∏ NeuroGraph 1.0.0*# NeuroGraph Developer Guide