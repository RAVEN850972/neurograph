"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –º–µ–∂–¥—É –º–æ–¥—É–ª—è–º–∏.
"""

import time
import asyncio
from typing import List, Dict, Any
from neurograph.integration import (
    NeuroGraphEngine,
    ComponentProvider,
    ProcessingRequest,
    IntegrationConfig
)


class CustomKnowledgeSystem:
    """–ü—Ä–∏–º–µ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π —Å–∏—Å—Ç–µ–º—ã –∑–Ω–∞–Ω–∏–π –Ω–∞ –±–∞–∑–µ NeuroGraph."""
    
    def __init__(self, config_path: str = None):
        self.config = self._create_custom_config()
        self.provider = ComponentProvider()
        self.engine = NeuroGraphEngine(self.provider)
        
        if not self.engine.initialize(self.config):
            raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–∏—Å—Ç–µ–º—É")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–±–æ—Ç—ã
        self.stats = {
            "documents_processed": 0,
            "queries_answered": 0,
            "knowledge_items": 0,
            "start_time": time.time()
        }
    
    def _create_custom_config(self) -> IntegrationConfig:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        return IntegrationConfig(
            engine_name="custom_knowledge_system",
            components={
                "semgraph": {
                    "type": "persistent",
                    "params": {
                        "file_path": "knowledge_base.json",
                        "auto_save_interval": 180.0
                    }
                },
                "memory": {
                    "params": {
                        "stm_capacity": 150,
                        "ltm_capacity": 15000,
                        "use_semantic_indexing": True,
                        "auto_consolidation": True
                    }
                },
                "processor": {
                    "type": "pattern_matching",
                    "params": {
                        "confidence_threshold": 0.6,
                        "enable_explanations": True
                    }
                }
            },
            max_concurrent_requests=15,
            enable_caching=True,
            enable_metrics=True
        )
    
    def process_document(self, document_text: str, document_title: str = None) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
        request = ProcessingRequest(
            content=document_text,
            request_type="learning",
            metadata={
                "document_title": document_title,
                "processing_timestamp": time.time()
            }
        )
        
        response = self.engine.process_request(request)
        
        if response.success:
            self.stats["documents_processed"] += 1
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            nlp_data = response.structured_data.get("nlp", {})
            graph_data = response.structured_data.get("graph", {})
            
            self.stats["knowledge_items"] += len(nlp_data.get("entities", []))
        
        return {
            "success": response.success,
            "entities_found": len(nlp_data.get("entities", [])),
            "relations_found": len(nlp_data.get("relations", [])),
            "processing_time": response.processing_time,
            "error": response.error_message
        }
    
    def answer_question(self, question: str) -> Dict[str, Any]:
        """–û—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
        request = ProcessingRequest(
            content=question,
            request_type="query",
            response_format="conversational",
            explanation_level="detailed"
        )
        
        response = self.engine.process_request(request)
        
        if response.success:
            self.stats["queries_answered"] += 1
        
        return {
            "answer": response.primary_response,
            "confidence": response.confidence,
            "sources": response.sources,
            "explanation": response.explanation,
            "processing_time": response.processing_time,
            "success": response.success
        }
    
    def get_knowledge_graph_summary(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –ø–æ –≥—Ä–∞—Ñ—É –∑–Ω–∞–Ω–∏–π."""
        try:
            graph = self.provider.get_component("semgraph")
            memory = self.provider.get_component("memory")
            
            return {
                "graph_nodes": len(graph.get_all_nodes()),
                "graph_edges": len(graph.get_all_edges()),
                "memory_size": memory.size(),
                "memory_stats": memory.get_memory_statistics()
            }
        except Exception as e:
            return {"error": str(e)}
    
    def get_system_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã."""
        uptime = time.time() - self.stats["start_time"]
        
        base_stats = self.stats.copy()
        base_stats["uptime_seconds"] = uptime
        base_stats["engine_health"] = self.engine.get_health_status()
        base_stats["knowledge_summary"] = self.get_knowledge_graph_summary()
        
        return base_stats
    
    def shutdown(self):
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã."""
        return self.engine.shutdown()


def example_custom_knowledge_system():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π —Å–∏—Å—Ç–µ–º—ã –∑–Ω–∞–Ω–∏–π."""
    print("=== –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –∑–Ω–∞–Ω–∏–π ===")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
    knowledge_system = CustomKnowledgeSystem()
    
    # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
    documents = [
        ("Python", 
         "Python - –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—â–µ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è. "
         "–û–Ω –±—ã–ª —Å–æ–∑–¥–∞–Ω –ì–≤–∏–¥–æ –≤–∞–Ω –†–æ—Å—Å—É–º–æ–º –≤ 1991 –≥–æ–¥—É. Python —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è "
         "–¥–ª—è –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏, –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è."),
        
        ("Django",
         "Django - —ç—Ç–æ –≤–µ–±-—Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –¥–ª—è Python, –∫–æ—Ç–æ—Ä—ã–π —Å–ø–æ—Å–æ–±—Å—Ç–≤—É–µ—Ç "
         "–±—ã—Å—Ç—Ä–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –∏ —á–∏—Å—Ç–æ–º—É, –ø—Ä–∞–≥–º–∞—Ç–∏—á–Ω–æ–º—É –¥–∏–∑–∞–π–Ω—É. –û–Ω –±—ã–ª —Å–æ–∑–¥–∞–Ω –≤ 2003 –≥–æ–¥—É "
         "–∏ —Å–ª–µ–¥—É–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø—É DRY (Don't Repeat Yourself)."),
        
        ("–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
         "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ –æ–±–ª–∞—Å—Ç—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è –¥–∞–µ—Ç "
         "–∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –æ–±—É—á–∞—Ç—å—Å—è –±–µ–∑ —è–≤–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –û–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç "
         "–∞–ª–≥–æ—Ä–∏—Ç–º—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö, –≤—ã—è–≤–ª–µ–Ω–∏—è –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–µ–π –∏ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π."),
        
        ("TensorFlow",
         "TensorFlow - —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è Google. "
         "–û–Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤—Å–µ–æ–±—ä–µ–º–ª—é—â—É—é —ç–∫–æ—Å–∏—Å—Ç–µ–º—É –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –±–∏–±–ª–∏–æ—Ç–µ–∫ –∏ —Ä–µ—Å—É—Ä—Å–æ–≤ "
         "–¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.")
    ]
    
    print("–û–±—É—á–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö...")
    for title, content in documents:
        result = knowledge_system.process_document(content, title)
        print(f"üìÑ {title}: {'‚úì' if result['success'] else '‚úó'} "
              f"({result['entities_found']} —Å—É—â–Ω–æ—Å—Ç–µ–π, {result['processing_time']:.3f}—Å)")
    
    # –ó–∞–¥–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã
    questions = [
        "–ß—Ç–æ —Ç–∞–∫–æ–µ Python?",
        "–ö—Ç–æ —Å–æ–∑–¥–∞–ª Python?",
        "–î–ª—è —á–µ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Django?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?",
        "–ö–∞–∫–∞—è —Å–≤—è–∑—å –º–µ–∂–¥—É Python –∏ –º–∞—à–∏–Ω–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º?",
        "–†–∞—Å—Å–∫–∞–∂–∏ –æ TensorFlow"
    ]
    
    print(f"\n–û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã:")
    for question in questions:
        result = knowledge_system.answer_question(question)
        print(f"\n‚ùì {question}")
        print(f"üí¨ {result['answer']}")
        if result['confidence'] > 0:
            print(f"üéØ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.2f}")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è: {result['processing_time']:.3f}—Å")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã
    print(f"\n=== –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã ===")
    stats = knowledge_system.get_system_statistics()
    print(f"üìö –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['documents_processed']}")
    print(f"‚ùì –í–æ–ø—Ä–æ—Å–æ–≤ –æ—Ç–≤–µ—á–µ–Ω–æ: {stats['queries_answered']}")
    print(f"üß† –≠–ª–µ–º–µ–Ω—Ç–æ–≤ –∑–Ω–∞–Ω–∏–π: {stats['knowledge_items']}")
    print(f"‚è∞ –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: {stats['uptime_seconds']:.1f}—Å")
    
    knowledge_summary = stats['knowledge_summary']
    if 'graph_nodes' in knowledge_summary:
        print(f"üï∏Ô∏è –£–∑–ª–æ–≤ –≤ –≥—Ä–∞—Ñ–µ: {knowledge_summary['graph_nodes']}")
        print(f"üîó –°–≤—è–∑–µ–π –≤ –≥—Ä–∞—Ñ–µ: {knowledge_summary['graph_edges']}")
        print(f"üíæ –†–∞–∑–º–µ—Ä –ø–∞–º—è—Ç–∏: {knowledge_summary['memory_size']}")
    
    # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
    knowledge_system.shutdown()


class ConversationalAgent:
    """–ü—Ä–∏–º–µ—Ä —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –Ω–∞ –±–∞–∑–µ NeuroGraph."""
    
    def __init__(self):
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤
        config = IntegrationConfig(
            engine_name="conversational_agent",
            components={
                "nlp": {
                    "params": {
                        "language": "ru",
                        "conversation_mode": True
                    }
                },
                "memory": {
                    "params": {
                        "stm_capacity": 50,  # –ë–æ–ª—å—à–µ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–∏–∞–ª–æ–≥–∞
                        "ltm_capacity": 5000,
                        "conversation_memory": True
                    }
                },
                "processor": {
                    "type": "pattern_matching",
                    "params": {
                        "confidence_threshold": 0.4,  # –ù–∏–∂–µ –¥–ª—è —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤
                        "conversation_rules": True
                    }
                }
            },
            max_concurrent_requests=5,
            enable_caching=True
        )
        
        self.provider = ComponentProvider()
        self.engine = NeuroGraphEngine(self.provider)
        self.engine.initialize(config)
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞
        self.conversation_context = {
            "user_name": None,
            "topics_discussed": [],
            "last_responses": [],
            "session_start": time.time()
        }
    
    def chat(self, user_input: str, user_name: str = None) -> str:
        """–î–∏–∞–ª–æ–≥ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º."""
        if user_name:
            self.conversation_context["user_name"] = user_name
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–∏–∞–ª–æ–≥–∞
        request = ProcessingRequest(
            content=user_input,
            request_type="query",
            response_format="conversational",
            context=self.conversation_context.copy(),
            metadata={
                "conversation_turn": len(self.conversation_context["last_responses"]),
                "user_name": self.conversation_context["user_name"]
            }
        )
        
        response = self.engine.process_request(request)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞
        self._update_conversation_context(user_input, response.primary_response)
        
        return response.primary_response
    
    def _update_conversation_context(self, user_input: str, agent_response: str):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–∏–∞–ª–æ–≥–∞."""
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ø–∏–∫–∏ (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
        words = user_input.lower().split()
        important_words = [w for w in words if len(w) > 4 and w not in ['–∫–æ—Ç–æ—Ä—ã–π', '–ø–æ—Ç–æ–º—É', '—Å–∫–∞–∑–∞—Ç—å']]
        self.conversation_context["topics_discussed"].extend(important_words[:2])
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if len(self.conversation_context["topics_discussed"]) > 20:
            self.conversation_context["topics_discussed"] = (
                self.conversation_context["topics_discussed"][-10:]
            )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ä–µ–ø–ª–∏–∫–∏
        self.conversation_context["last_responses"].append({
            "user": user_input,
            "agent": agent_response,
            "timestamp": time.time()
        })
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
        if len(self.conversation_context["last_responses"]) > 5:
            self.conversation_context["last_responses"] = (
                self.conversation_context["last_responses"][-3:]
            )
    
    def get_conversation_summary(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –ø–æ –¥–∏–∞–ª–æ–≥—É."""
        duration = time.time() - self.conversation_context["session_start"]
        
        return {
            "user_name": self.conversation_context["user_name"],
            "duration_seconds": duration,
            "turns_count": len(self.conversation_context["last_responses"]),
            "topics_discussed": list(set(self.conversation_context["topics_discussed"])),
            "last_interaction": (
                self.conversation_context["last_responses"][-1]["timestamp"]
                if self.conversation_context["last_responses"] else None
            )
        }
    
    def shutdown(self):
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞."""
        return self.engine.shutdown()


def example_conversational_agent():
    """–ü—Ä–∏–º–µ—Ä —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞."""
    print("\n=== –†–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–π –∞–≥–µ–Ω—Ç ===")
    
    agent = ConversationalAgent()
    
    # –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∞–µ–º –∞–≥–µ–Ω—Ç–∞ –±–∞–∑–æ–≤—ã–º –∑–Ω–∞–Ω–∏—è–º
    agent.engine.learn("–ú–µ–Ω—è –∑–æ–≤—É—Ç NeuroGraph. –Ø - –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.")
    agent.engine.learn("–Ø –ø–æ–º–æ–≥–∞—é –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.")
    agent.engine.learn("Python - —ç—Ç–æ —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è.")
    
    # –î–∏–∞–ª–æ–≥
    conversation = [
        ("–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?", "–ê–ª–∏—Å–∞"),
        ("–†–∞—Å—Å–∫–∞–∂–∏ –æ —Å–µ–±–µ", None),
        ("–ß—Ç–æ —Ç—ã –∑–Ω–∞–µ—à—å –æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏?", None),
        ("–ê —á—Ç–æ —Ç–∞–∫–æ–µ Python?", None),
        ("–°–ø–∞—Å–∏–±–æ –∑–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é!", None),
        ("–î–æ —Å–≤–∏–¥–∞–Ω–∏—è!", None)
    ]
    
    print("üí¨ –ù–∞—á–∏–Ω–∞–µ–º –¥–∏–∞–ª–æ–≥:")
    for user_message, user_name in conversation:
        print(f"\nüë§ {user_name or '–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å'}: {user_message}")
        
        response = agent.chat(user_message, user_name)
        print(f"ü§ñ NeuroGraph: {response}")
        
        time.sleep(0.5)  # –ò–º–∏—Ç–∞—Ü–∏—è –ø–∞—É–∑—ã –≤ –¥–∏–∞–ª–æ–≥–µ
    
    # –°–≤–æ–¥–∫–∞ –ø–æ –¥–∏–∞–ª–æ–≥—É
    summary = agent.get_conversation_summary()
    print(f"\n=== –°–≤–æ–¥–∫–∞ –ø–æ –¥–∏–∞–ª–æ–≥—É ===")
    print(f"üë§ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {summary['user_name'] or '–ê–Ω–æ–Ω–∏–º–Ω—ã–π'}")
    print(f"‚è∞ –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {summary['duration_seconds']:.1f}—Å")
    print(f"üí¨ –†–µ–ø–ª–∏–∫: {summary['turns_count']}")
    print(f"üìù –û–±—Å—É–∂–¥–∞–≤—à–∏–µ—Å—è —Ç–µ–º—ã: {', '.join(summary['topics_discussed'][:5])}")
    
    agent.shutdown()


class BatchProcessor:
    """–ü—Ä–∏–º–µ—Ä –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
    
    def __init__(self):
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        config = IntegrationConfig(
            engine_name="batch_processor",
            components={
                "nlp": {
                    "params": {
                        "language": "ru",
                        "batch_mode": True
                    }
                }
            },
            max_concurrent_requests=20,  # –ë–æ–ª—å—à–µ –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            enable_caching=True,
            enable_metrics=True
        )
        
        self.provider = ComponentProvider()
        self.engine = NeuroGraphEngine(self.provider)
        self.engine.initialize(config)
        
        self.processing_stats = {
            "total_processed": 0,
            "successful": 0,
            "failed": 0,
            "total_time": 0.0
        }
    
    def process_batch(self, documents: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
        results = []
        start_time = time.time()
        
        print(f"üì¶ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞–∫–µ—Ç–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        
        for i, doc in enumerate(documents, 1):
            try:
                doc_start = time.time()
                
                request = ProcessingRequest(
                    content=doc["content"],
                    request_type="learning",
                    metadata={
                        "title": doc.get("title", f"–î–æ–∫—É–º–µ–Ω—Ç {i}"),
                        "batch_id": doc.get("id", str(i)),
                        "batch_position": i
                    }
                )
                
                response = self.engine.process_request(request)
                doc_time = time.time() - doc_start
                
                result = {
                    "id": doc.get("id", str(i)),
                    "title": doc.get("title", f"–î–æ–∫—É–º–µ–Ω—Ç {i}"),
                    "success": response.success,
                    "processing_time": doc_time,
                    "entities_count": 0,
                    "relations_count": 0
                }
                
                if response.success:
                    nlp_data = response.structured_data.get("nlp", {})
                    result["entities_count"] = len(nlp_data.get("entities", []))
                    result["relations_count"] = len(nlp_data.get("relations", []))
                    self.processing_stats["successful"] += 1
                else:
                    result["error"] = response.error_message
                    self.processing_stats["failed"] += 1
                
                results.append(result)
                self.processing_stats["total_processed"] += 1
                
                # –ü—Ä–æ–≥—Ä–µ—Å—Å
                if i % 5 == 0 or i == len(documents):
                    progress = (i / len(documents)) * 100
                    print(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}% ({i}/{len(documents)})")
                
            except Exception as e:
                results.append({
                    "id": doc.get("id", str(i)),
                    "title": doc.get("title", f"–î–æ–∫—É–º–µ–Ω—Ç {i}"),
                    "success": False,
                    "error": str(e),
                    "processing_time": 0.0
                })
                self.processing_stats["failed"] += 1
        
        self.processing_stats["total_time"] = time.time() - start_time
        return results
    
    def get_batch_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
        total = self.processing_stats["total_processed"]
        avg_time = (
            self.processing_stats["total_time"] / total
            if total > 0 else 0
        )
        
        return {
            "total_documents": total,
            "successful": self.processing_stats["successful"],
            "failed": self.processing_stats["failed"],
            "success_rate": (
                self.processing_stats["successful"] / total
                if total > 0 else 0
            ),
            "total_time": self.processing_stats["total_time"],
            "average_time_per_document": avg_time,
            "throughput_docs_per_second": (
                total / self.processing_stats["total_time"]
                if self.processing_stats["total_time"] > 0 else 0
            )
        }
    
    def shutdown(self):
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞."""
        return self.engine.shutdown()


def example_batch_processing():
    """–ü—Ä–∏–º–µ—Ä –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
    print("\n=== –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ ===")
    
    processor = BatchProcessor()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    documents = [
        {
            "id": "doc_1",
            "title": "–û—Å–Ω–æ–≤—ã Python",
            "content": "Python - –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–π —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è. "
                      "–û–Ω –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é —Ç–∏–ø–∏–∑–∞—Ü–∏—é."
        },
        {
            "id": "doc_2", 
            "title": "–í–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞",
            "content": "Django –∏ Flask - –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –≤–µ–±-—Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ –¥–ª—è Python. "
                      "–û–Ω–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç –±—ã—Å—Ç—Ä–æ —Å–æ–∑–¥–∞–≤–∞—Ç—å –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è."
        },
        {
            "id": "doc_3",
            "title": "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
            "content": "TensorFlow –∏ PyTorch - –≤–µ–¥—É—â–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. "
                      "–û–Ω–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π."
        },
        {
            "id": "doc_4",
            "title": "–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö", 
            "content": "Pandas –∏ NumPy - –æ—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –≤ Python. "
                      "–û–Ω–∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Ä–∞–±–æ—Ç—É —Å –±–æ–ª—å—à–∏–º–∏ –Ω–∞–±–æ—Ä–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö."
        },
        {
            "id": "doc_5",
            "title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
            "content": "–ò–ò –≤–∫–ª—é—á–∞–µ—Ç –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –æ–±—Ä–∞–±–æ—Ç–∫—É –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ. "
                      "Python —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ò–ò-—Å–∏—Å—Ç–µ–º."
        }
    ]
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞
    results = processor.process_batch(documents)
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ ===")
    for result in results:
        status = "‚úì" if result["success"] else "‚úó"
        print(f"{status} {result['title']}: {result['entities_count']} —Å—É—â–Ω–æ—Å—Ç–µ–π, "
              f"{result['relations_count']} –æ—Ç–Ω–æ—à–µ–Ω–∏–π ({result['processing_time']:.3f}—Å)")
        
        if not result["success"] and "error" in result:
            print(f"   –û—à–∏–±–∫–∞: {result['error']}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = processor.get_batch_statistics()
    print(f"\n=== –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ ===")
    print(f"üìÑ –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {stats['total_documents']}")
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {stats['successful']}")
    print(f"‚ùå –û—à–∏–±–æ–∫: {stats['failed']}")
    print(f"üìä –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {stats['success_rate']:.1%}")
    print(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {stats['total_time']:.2f}—Å")
    print(f"üöÄ –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç: {stats['average_time_per_document']:.3f}—Å")
    print(f"üìà –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {stats['throughput_docs_per_second']:.1f} –¥–æ–∫/—Å")
    
    processor.shutdown()


def main():
    """–ó–∞–ø—É—Å–∫ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤."""
    print("üöÄ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ NeuroGraph\n")
    
    examples = [
        ("–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –∑–Ω–∞–Ω–∏–π", example_custom_knowledge_system),
        ("–†–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–π –∞–≥–µ–Ω—Ç", example_conversational_agent),
        ("–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", example_batch_processing)
    ]
    
    for name, example_func in examples:
        print(f"üìã {name}")
        try:
            example_func()
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø—Ä–∏–º–µ—Ä–µ '{name}': {e}")
        
        print("\n" + "="*70)
    
    print("‚úÖ –í—Å–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")


if __name__ == "__main__":
    main()