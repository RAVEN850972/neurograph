from flask import Flask, render_template, request, jsonify, send_file
from flask_cors import CORS
import json
import uuid
import time
import os
import tempfile
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import threading
import asyncio

# –ò–º–ø–æ—Ä—Ç –Ω–∞—à–µ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –∏ —Å–∏—Å—Ç–µ–º—ã –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
try:
   from neurograph.integration import create_default_engine
   NEUROGRAPH_AVAILABLE = True
except ImportError:
   NEUROGRAPH_AVAILABLE = False
   print("‚ö†Ô∏è NeuroGraph –Ω–µ –Ω–∞–π–¥–µ–Ω, —Ä–∞–±–æ—Ç–∞–µ–º –≤ –¥–µ–º–æ-—Ä–µ–∂–∏–º–µ")

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º—É –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
try:
   from bulk_learning import BulkLearningSystem, TextChunker
   BULK_LEARNING_AVAILABLE = True
except ImportError:
   BULK_LEARNING_AVAILABLE = False
   print("‚ö†Ô∏è –°–∏—Å—Ç–µ–º–∞ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")

app = Flask(__name__)
CORS(app)

class WebNeuroGraphAssistant:
   """–í–µ–±-–≤–µ—Ä—Å–∏—è –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ NeuroGraph."""
   
   def __init__(self):
       self.user_sessions = {}  # –•—Ä–∞–Ω–µ–Ω–∏–µ —Å–µ—Å—Å–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
       self.global_stats = {
           "total_users": 0,
           "total_messages": 0,
           "start_time": datetime.now(),
           "system_healthy": NEUROGRAPH_AVAILABLE
       }
       
       # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NeuroGraph
       if NEUROGRAPH_AVAILABLE:
           try:
               self.engine = create_default_engine()
               self.memory = self.engine.provider.get_component('memory')
               self.graph = self.engine.provider.get_component('semgraph')
               
               # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
               if BULK_LEARNING_AVAILABLE:
                   self.bulk_system = BulkLearningSystem(
                       neurograph_engine=self.engine,
                       batch_size=5,
                       delay_between_batches=0.1,
                       progress_callback=self._bulk_progress_callback
                   )
               else:
                   self.bulk_system = None
               
               print("‚úÖ NeuroGraph —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
           except Exception as e:
               print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ NeuroGraph: {e}")
               self.engine = None
               self.memory = None
               self.graph = None
               self.bulk_system = None
               self.global_stats["system_healthy"] = False
       else:
           self.engine = None
           self.memory = None
           self.graph = None
           self.bulk_system = None
       
       # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
       self.bulk_progress = {}
       self.bulk_tasks = {}  # –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á
   
   def _bulk_progress_callback(self, message: str):
       """Callback –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è."""
       timestamp = datetime.now().isoformat()
       
       # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—â–∏–π –ª–æ–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
       if 'bulk_learning' not in self.bulk_progress:
           self.bulk_progress['bulk_learning'] = []
       
       self.bulk_progress['bulk_learning'].append({
           'timestamp': timestamp,
           'message': message
       })
       
       # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –ª–æ–≥–∞
       if len(self.bulk_progress['bulk_learning']) > 100:
           self.bulk_progress['bulk_learning'] = self.bulk_progress['bulk_learning'][-50:]
       
       print(f"[BULK] {message}")
   
   def process_bulk_text(self, text: str, session_id: str, 
                        chunk_size: int = 500, 
                        importance: float = 0.7,
                        category: str = "bulk_text") -> Dict:
       """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–æ–ª—å—à–æ–π —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ —Å–∏—Å—Ç–µ–º—É –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è."""
       
       if not self.bulk_system:
           return {
               "success": False,
               "error": "–°–∏—Å—Ç–µ–º–∞ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"
           }
       
       task_id = str(uuid.uuid4())
       
       try:
           # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º chunker
           self.bulk_system.chunker.chunk_size = chunk_size
           
           # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
           metadata = {
               'session_id': session_id,
               'category': category,
               'task_id': task_id,
               'text_length': len(text)
           }
           
           # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
           self.bulk_tasks[task_id] = {
               'start_time': datetime.now(),
               'status': 'processing',
               'session_id': session_id
           }
           
           # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç
           result = self.bulk_system.process_large_text(
               text=text,
               metadata=metadata,
               importance=importance
           )
           
           # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏
           self.bulk_tasks[task_id].update({
               'status': 'completed' if result['success'] else 'failed',
               'end_time': datetime.now(),
               'result': result
           })
           
           # –û–±–Ω–æ–≤–ª—è–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –±–∞–∑—É —Å–µ—Å—Å–∏–∏
           session = self.get_or_create_session(session_id)
           session["total_facts"] += result.get('processed_chunks', 0)
           
           return {
               "success": True,
               "task_id": task_id,
               "result": result
           }
           
       except Exception as e:
           # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ
           if task_id in self.bulk_tasks:
               self.bulk_tasks[task_id].update({
                   'status': 'failed',
                   'end_time': datetime.now(),
                   'error': str(e)
               })
           
           return {
               "success": False,
               "task_id": task_id,
               "error": str(e)
           }
   
   def process_bulk_file(self, file_content: str, filename: str, 
                        session_id: str, **kwargs) -> Dict:
       """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª."""
       
       if not self.bulk_system:
           return {
               "success": False,
               "error": "–°–∏—Å—Ç–µ–º–∞ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"
           }
       
       task_id = str(uuid.uuid4())
       
       try:
           # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é —Ñ–∞–π–ª–∞
           file_ext = os.path.splitext(filename)[1].lower()
           category_map = {
               '.txt': 'text_document',
               '.md': 'markdown',
               '.py': 'code_python',
               '.js': 'code_javascript',
               '.html': 'html_document',
               '.json': 'json_data'
           }
           category = category_map.get(file_ext, 'document')
           
           # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
           metadata = {
               'filename': filename,
               'file_extension': file_ext,
               'session_id': session_id,
               'task_id': task_id,
               'upload_time': datetime.now().isoformat()
           }
           
           # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
           self.bulk_tasks[task_id] = {
               'start_time': datetime.now(),
               'status': 'processing',
               'session_id': session_id,
               'filename': filename
           }
           
           result = self.bulk_system.process_large_text(
               text=file_content,
               metadata=metadata,
               importance=kwargs.get('importance', 0.7)
           )
           
           # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏
           self.bulk_tasks[task_id].update({
               'status': 'completed' if result['success'] else 'failed',
               'end_time': datetime.now(),
               'result': result
           })
           
           # –û–±–Ω–æ–≤–ª—è–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –±–∞–∑—É —Å–µ—Å—Å–∏–∏
           session = self.get_or_create_session(session_id)
           session["total_facts"] += result.get('processed_chunks', 0)
           
           return {
               "success": True,
               "task_id": task_id,
               "filename": filename,
               "result": result
           }
           
       except Exception as e:
           if task_id in self.bulk_tasks:
               self.bulk_tasks[task_id].update({
                   'status': 'failed',
                   'end_time': datetime.now(),
                   'error': str(e)
               })
           
           return {
               "success": False,
               "task_id": task_id,
               "filename": filename,
               "error": str(e)
           }
   
   def get_bulk_task_status(self, task_id: str) -> Dict:
       """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è."""
       
       if task_id not in self.bulk_tasks:
           return {
               "success": False,
               "error": "–ó–∞–¥–∞—á–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
           }
       
       task_info = self.bulk_tasks[task_id].copy()
       
       # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
       if 'start_time' in task_info:
           task_info['start_time'] = task_info['start_time'].isoformat()
       if 'end_time' in task_info:
           task_info['end_time'] = task_info['end_time'].isoformat()
           
           # –í—ã—á–∏—Å–ª—è–µ–º –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
           start = self.bulk_tasks[task_id]['start_time']
           end = self.bulk_tasks[task_id]['end_time']
           duration = (end - start).total_seconds()
           task_info['duration_seconds'] = duration
       
       return {
           "success": True,
           "task": task_info
       }
   
   def get_bulk_progress_log(self, limit: int = 50) -> List[Dict]:
       """–ü–æ–ª—É—á–∞–µ—Ç –ª–æ–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è."""
       
       if 'bulk_learning' not in self.bulk_progress:
           return []
       
       return self.bulk_progress['bulk_learning'][-limit:]
   
   def cleanup_old_tasks(self, max_age_hours: int = 24):
       """–û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ –∑–∞–¥–∞—á–∏."""
       
       cutoff_time = datetime.now() - timedelta(hours=max_age_hours)
       tasks_to_remove = []
       
       for task_id, task_info in self.bulk_tasks.items():
           if task_info.get('end_time') and task_info['end_time'] < cutoff_time:
               tasks_to_remove.append(task_id)
       
       for task_id in tasks_to_remove:
           del self.bulk_tasks[task_id]
       
       return len(tasks_to_remove)
   
   def get_or_create_session(self, session_id: str = None) -> Dict:
       """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫—É—é —Å–µ—Å—Å–∏—é."""
       if not session_id:
           session_id = str(uuid.uuid4())
       
       if session_id not in self.user_sessions:
           self.user_sessions[session_id] = {
               "id": session_id,
               "created_at": datetime.now(),
               "messages": [],
               "local_knowledge": {},
               "user_name": f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å_{len(self.user_sessions) + 1}",
               "total_queries": 0,
               "total_facts": 0
           }
           self.global_stats["total_users"] += 1
           
           # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
           welcome_msg = {
               "id": str(uuid.uuid4()),
               "sender": "assistant",
               "content": f"–ü—Ä–∏–≤–µ—Ç! –Ø NeuroGraph Assistant - –≤–∞—à –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –ò–ò-–ø–æ–º–æ—â–Ω–∏–∫ —Å –±–∏–æ–º–æ—Ä—Ñ–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π –ø–∞–º—è—Ç–∏. –Ø –º–æ–≥—É –æ–±—É—á–∞—Ç—å—Å—è, –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –ø–æ–º–æ–≥–∞—Ç—å —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á–∏. –û —á–µ–º —Ö–æ—Ç–∏—Ç–µ –ø–æ–≥–æ–≤–æ—Ä–∏—Ç—å?",
               "timestamp": datetime.now().isoformat(),
               "confidence": 0.95,
               "sources": ["system"],
               "processing_time": 0
           }
           self.user_sessions[session_id]["messages"].append(welcome_msg)
       
       return self.user_sessions[session_id]
   
   async def process_message(self, session_id: str, message: str) -> Dict:
       """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
       session = self.get_or_create_session(session_id)
       start_time = time.time()
       
       # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –∏—Å—Ç–æ—Ä–∏—é
       user_msg = {
           "id": str(uuid.uuid4()),
           "sender": "user",
           "content": message,
           "timestamp": datetime.now().isoformat(),
           "confidence": 1.0
       }
       session["messages"].append(user_msg)
       session["total_queries"] += 1
       self.global_stats["total_messages"] += 1
       
       try:
           # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ NeuroGraph –∏–ª–∏ fallback
           if self.engine:
               response = await self._process_with_neurograph(message, session)
           else:
               response = self._process_with_fallback(message, session)
           
           processing_time = (time.time() - start_time) * 1000
           
           # –°–æ–∑–¥–∞–µ–º –æ—Ç–≤–µ—Ç–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
           assistant_msg = {
               "id": str(uuid.uuid4()),
               "sender": "assistant",
               "content": response["answer"],
               "timestamp": datetime.now().isoformat(),
               "confidence": response.get("confidence", 0.5),
               "sources": response.get("sources", []),
               "processing_time": processing_time,
               "meta": response.get("meta", {})
           }
           
           session["messages"].append(assistant_msg)
           
           return {
               "success": True,
               "message": assistant_msg,
               "session_stats": self._get_session_stats(session)
           }
           
       except Exception as e:
           error_msg = {
               "id": str(uuid.uuid4()),
               "sender": "assistant",
               "content": f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è: {str(e)}",
               "timestamp": datetime.now().isoformat(),
               "confidence": 0.1,
               "sources": ["error_handler"],
               "processing_time": (time.time() - start_time) * 1000,
               "error": True
           }
           
           session["messages"].append(error_msg)
           
           return {
               "success": False,
               "message": error_msg,
               "error": str(e)
           }
   
   async def _process_with_neurograph(self, message: str, session: Dict) -> Dict:
       """–û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ NeuroGraph."""
       try:
           # –°–∏–º—É–ª—è—Ü–∏—è async –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
           await self._async_delay(0.1)
           
           # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –∫–æ–º–∞–Ω–¥
           if any(cmd in message.lower() for cmd in ['–∑–∞–ø–æ–º–Ω–∏', '–æ–±—É—á–∏', '—Å–æ—Ö—Ä–∞–Ω–∏']):
               return await self._handle_learning_command(message, session)
           
           if any(cmd in message.lower() for cmd in ['–ø–æ–∫–∞–∂–∏ –ø–∞–º—è—Ç—å', '—á—Ç–æ –ø–æ–º–Ω–∏—à—å', '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏']):
               return await self._handle_memory_command(session)
           
           if any(cmd in message.lower() for cmd in ['—Å—Ç–∞—Ç—É—Å', '—Å–∏—Å—Ç–µ–º–∞', '–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞']):
               return await self._handle_system_command()
           
           # –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å –∫ NeuroGraph
           response = self.engine.query(message)
           
           # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
           if (hasattr(response, 'confidence') and response.confidence < 0.3) or \
              "–Ω–µ –Ω–∞—à–µ–ª" in response.primary_response.lower():
               
               fallback_response = self._search_local_knowledge(message, session)
               if fallback_response["confidence"] > response.confidence:
                   return {
                       "answer": f"{fallback_response['answer']}\n\nüí° *–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –æ—Ç NeuroGraph: {response.primary_response}*",
                       "confidence": max(fallback_response["confidence"], response.confidence),
                       "sources": ["–ª–æ–∫–∞–ª—å–Ω–∞—è –±–∞–∑–∞", "NeuroGraph"],
                       "meta": {"combined_response": True}
                   }
           
           return {
               "answer": response.primary_response,
               "confidence": getattr(response, 'confidence', 0.7),
               "sources": getattr(response, 'sources', ["NeuroGraph"]),
               "meta": {
                   "neurograph_response": True,
                   "processing_time": getattr(response, 'processing_time', 0)
               }
           }
           
       except Exception as e:
           print(f"–û—à–∏–±–∫–∞ NeuroGraph: {e}")
           return self._process_with_fallback(message, session)
   
   async def _handle_learning_command(self, message: str, session: Dict) -> Dict:
       """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã –æ–±—É—á–µ–Ω–∏—è."""
       # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
       content = message
       for trigger in ['–∑–∞–ø–æ–º–Ω–∏', '–æ–±—É—á–∏', '—Å–æ—Ö—Ä–∞–Ω–∏']:
           content = content.replace(trigger, '').strip()
       
       if not content:
           return {
               "answer": "–ß—Ç–æ –∏–º–µ–Ω–Ω–æ –≤—ã —Ö–æ—Ç–∏—Ç–µ, —á—Ç–æ–±—ã —è –∑–∞–ø–æ–º–Ω–∏–ª?",
               "confidence": 0.9,
               "sources": ["learning_system"]
           }
       
       # –û–±—É—á–∞–µ–º NeuroGraph
       if self.engine:
           response = self.engine.learn(content)
           if response.success:
               # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ª–æ–∫–∞–ª—å–Ω—É—é –±–∞–∑—É —Å–µ—Å—Å–∏–∏
               fact_id = str(uuid.uuid4())
               session["local_knowledge"][fact_id] = {
                   "content": content,
                   "timestamp": datetime.now().isoformat(),
                   "category": "user_taught"
               }
               session["total_facts"] += 1
               
               return {
                   "answer": f"‚úÖ –û—Ç–ª–∏—á–Ω–æ! –Ø –∑–∞–ø–æ–º–Ω–∏–ª: '{content}'\n\n–¢–µ–ø–µ—Ä—å —ç—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –º–æ–µ–π –ø–∞–º—è—Ç–∏ –∏ —è —Å–º–æ–≥—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ—ë –≤ –±—É–¥—É—â–∏—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–∞—Ö.",
                   "confidence": 0.95,
                   "sources": ["learning_system", "NeuroGraph"],
                   "meta": {"learned_content": content}
               }
       
       # Fallback: —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –ª–æ–∫–∞–ª—å–Ω–æ
       fact_id = str(uuid.uuid4())
       session["local_knowledge"][fact_id] = {
           "content": content,
           "timestamp": datetime.now().isoformat(),
           "category": "user_taught"
       }
       session["total_facts"] += 1
       
       return {
           "answer": f"üìù –ó–∞–ø–∏—Å–∞–ª –≤ –ª–æ–∫–∞–ª—å–Ω—É—é –ø–∞–º—è—Ç—å: '{content}'\n\n–ë—É–¥—É –ø–æ–º–Ω–∏—Ç—å —ç—Ç–æ –≤–æ –≤—Ä–µ–º—è –Ω–∞—à–µ–≥–æ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞!",
           "confidence": 0.8,
           "sources": ["local_memory"]
       }
   
   async def _handle_memory_command(self, session: Dict) -> Dict:
       """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã –ø–æ–∫–∞–∑–∞ –ø–∞–º—è—Ç–∏."""
       memory_info = []
       
       # –õ–æ–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞–Ω–∏—è —Å–µ—Å—Å–∏–∏
       local_count = len(session["local_knowledge"])
       if local_count > 0:
           memory_info.append(f"üìù **–õ–æ–∫–∞–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å —Å–µ—Å—Å–∏–∏**: {local_count} —Ñ–∞–∫—Ç–æ–≤")
           
           recent_facts = list(session["local_knowledge"].values())[-3:]
           for fact in recent_facts:
               memory_info.append(f"  ‚Ä¢ {fact['content']}")
       
       # NeuroGraph –ø–∞–º—è—Ç—å
       if self.memory:
           try:
               stats = self.memory.get_memory_statistics()
               stm_size = stats['memory_levels']['stm']['size']
               ltm_size = stats['memory_levels']['ltm']['size']
               
               memory_info.append(f"\nüß† **NeuroGraph –ø–∞–º—è—Ç—å**:")
               memory_info.append(f"  ‚Ä¢ –ö—Ä–∞—Ç–∫–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è: {stm_size} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
               memory_info.append(f"  ‚Ä¢ –î–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è: {ltm_size} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
               
               # –ü—Ä–æ–±—É–µ–º –ø–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –ø–∞–º—è—Ç–∏
               try:
                   recent_items = self.memory.get_recent_items()
                   if recent_items:
                       memory_info.append(f"\nüìã **–ü–æ—Å–ª–µ–¥–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã STM**:")
                       for item in recent_items[:3]:
                           content = getattr(item, 'content', str(item))
                           if len(content) > 50:
                               content = content[:50] + "..."
                           memory_info.append(f"  ‚Ä¢ {content}")
               except Exception as e:
                   memory_info.append(f"\n‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–∞–º—è—Ç–∏: {e}")
               
           except Exception as e:
               memory_info.append(f"\n‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ NeuroGraph –ø–∞–º—è—Ç–∏: {e}")
       
       if not memory_info:
           return {
               "answer": "–ú–æ—è –ø–∞–º—è—Ç—å –ø–æ–∫–∞ –ø—É—Å—Ç–∞. –†–∞—Å—Å–∫–∞–∂–∏—Ç–µ –º–Ω–µ —á—Ç–æ-–Ω–∏–±—É–¥—å, –∏ —è —ç—Ç–æ –∑–∞–ø–æ–º–Ω—é!",
               "confidence": 0.9,
               "sources": ["memory_system"]
           }
       
       return {
           "answer": "üß† **–°–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–µ–π –ø–∞–º—è—Ç–∏:**\n\n" + "\n".join(memory_info) + "\n\nüí° *–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç–µ—Å—å –Ω–∞ –≤–∫–ª–∞–¥–∫—É '–ó–Ω–∞–Ω–∏—è' –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞!*",
           "confidence": 0.95,
           "sources": ["memory_system"]
       }
   
   async def _handle_system_command(self) -> Dict:
       """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã —Å–∏—Å—Ç–µ–º–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏."""
       uptime = datetime.now() - self.global_stats["start_time"]
       uptime_str = str(uptime).split('.')[0]  # –£–±–∏—Ä–∞–µ–º –º–∏–∫—Ä–æ—Å–µ–∫—É–Ω–¥—ã
       
       status_info = [
           f"üñ•Ô∏è **–°–∏—Å—Ç–µ–º–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:**\n",
           f"‚è±Ô∏è **–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã**: {uptime_str}",
           f"üë• **–ê–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π**: {len(self.user_sessions)}",
           f"üí¨ **–í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π**: {self.global_stats['total_messages']}",
           f"üß† **NeuroGraph**: {'‚úÖ –ê–∫—Ç–∏–≤–µ–Ω' if self.engine else '‚ùå –ù–µ–¥–æ—Å—Ç—É–ø–µ–Ω'}",
       ]
       
       if self.memory:
           try:
               stats = self.memory.get_memory_statistics()
               total_items = stats.get('total_items', 0)
               status_info.append(f"üìä **–≠–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –ø–∞–º—è—Ç–∏**: {total_items}")
           except:
               pass
       
       status_info.append(f"\nüí° *–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç–µ—Å—å –Ω–∞ –≤–∫–ª–∞–¥–∫—É '–°–∏—Å—Ç–µ–º–∞' –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏!*")
       
       return {
           "answer": "\n".join(status_info),
           "confidence": 0.99,
           "sources": ["system_monitor"]
       }
   
   def _process_with_fallback(self, message: str, session: Dict) -> Dict:
       """Fallback –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ NeuroGraph."""
       # –ü–æ–∏—Å–∫ –≤ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞–Ω–∏—è—Ö
       local_response = self._search_local_knowledge(message, session)
       if local_response["confidence"] > 0.5:
           return local_response
       
       # –ü—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
       responses = {
           "–ø—Ä–∏–≤–µ—Ç": "–ü—Ä–∏–≤–µ—Ç! –†–∞–¥ –≤–∞—Å –≤–∏–¥–µ—Ç—å! –ö–∞–∫ –¥–µ–ª–∞?",
           "–∫–∞–∫ –¥–µ–ª–∞": "–£ –º–µ–Ω—è –≤—Å–µ –æ—Ç–ª–∏—á–Ω–æ! –ü—Ä–∞–≤–¥–∞, —Å–µ–π—á–∞—Å —Ä–∞–±–æ—Ç–∞—é –≤ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ, —Ç–∞–∫ –∫–∞–∫ NeuroGraph –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ù–æ —è –≤—Å–µ —Ä–∞–≤–Ω–æ –º–æ–≥—É –ø–æ–º–æ—á—å!",
           "—á—Ç–æ —Ç—ã —É–º–µ–µ—à—å": "–Ø —É–º–µ—é –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –±–µ—Å–µ–¥—É. –í –ø–æ–ª–Ω–æ–º —Ä–µ–∂–∏–º–µ —Å NeuroGraph —É –º–µ–Ω—è –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª—å—à–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π!",
           "–ø–æ–º–æ–≥–∏": "–ö–æ–Ω–µ—á–Ω–æ –ø–æ–º–æ–≥—É! –†–∞—Å—Å–∫–∞–∂–∏—Ç–µ, —á—Ç–æ –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç, –∏–ª–∏ –ø–æ–ø—Ä–æ—Å–∏—Ç–µ –º–µ–Ω—è —á—Ç–æ-—Ç–æ –∑–∞–ø–æ–º–Ω–∏—Ç—å.",
           "—Å–ø–∞—Å–∏–±–æ": "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞! –í—Å–µ–≥–¥–∞ —Ä–∞–¥ –ø–æ–º–æ—á—å! üòä"
       }
       
       message_lower = message.lower()
       for key, response in responses.items():
           if key in message_lower:
               return {
                   "answer": response,
                   "confidence": 0.8,
                   "sources": ["–ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã"]
               }
       
       # –û–±—â–∏–π –æ—Ç–≤–µ—Ç
       general_responses = [
           "–ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ! –†–∞—Å—Å–∫–∞–∂–∏—Ç–µ –±–æ–ª—å—à–µ –æ–± —ç—Ç–æ–º.",
           "–≠—Ç–æ —Ç—Ä–µ–±—É–µ—Ç —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π. –ú–æ–∂–µ—Ç–µ –¥–∞—Ç—å –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞?",
           "–•–º, –Ω–æ–≤–∞—è –¥–ª—è –º–µ–Ω—è —Ç–µ–º–∞. –î–∞–≤–∞–π—Ç–µ –∏–∑—É—á–∏–º –µ—ë –≤–º–µ—Å—Ç–µ!",
           "–Ø –ø–æ–∫–∞ –¥—É–º–∞—é –Ω–∞–¥ —ç—Ç–∏–º. –ê —á—Ç–æ –≤—ã —Å–∞–º–∏ –æ–± —ç—Ç–æ–º –∑–Ω–∞–µ—Ç–µ?"
       ]
       
       import random
       return {
           "answer": random.choice(general_responses),
           "confidence": 0.4,
           "sources": ["–æ–±—â–∏–µ –æ—Ç–≤–µ—Ç—ã"]
       }
   
   def _search_local_knowledge(self, query: str, session: Dict) -> Dict:
       """–ü–æ–∏—Å–∫ –≤ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞–Ω–∏—è—Ö —Å–µ—Å—Å–∏–∏."""
       if not session["local_knowledge"]:
           return {"answer": "", "confidence": 0.0, "sources": []}
       
       query_words = set(query.lower().split())
       relevant_facts = []
       
       for fact_data in session["local_knowledge"].values():
           content = fact_data["content"].lower()
           content_words = set(content.split())
           
           # –ü–æ–¥—Å—á–µ—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π
           overlap = len(query_words.intersection(content_words))
           if overlap > 0:
               relevant_facts.append({
                   "content": fact_data["content"],
                   "relevance": overlap,
                   "timestamp": fact_data["timestamp"]
               })
       
       if not relevant_facts:
           return {"answer": "", "confidence": 0.0, "sources": []}
       
       # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
       relevant_facts.sort(key=lambda x: x["relevance"], reverse=True)
       best_fact = relevant_facts[0]
       
       return {
           "answer": f"–ò–∑ —Ç–æ–≥–æ, —á—Ç–æ —è –ø–æ–º–Ω—é: {best_fact['content']}",
           "confidence": min(0.8, best_fact["relevance"] / 3),
           "sources": ["–ª–æ–∫–∞–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å —Å–µ—Å—Å–∏–∏"]
       }
   
   def _get_session_stats(self, session: Dict) -> Dict:
       """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–µ—Å—Å–∏–∏."""
       return {
           "session_id": session["id"],
           "messages_count": len(session["messages"]),
           "local_facts": len(session["local_knowledge"]),
           "total_queries": session["total_queries"],
           "total_facts": session["total_facts"],
           "session_duration": str(datetime.now() - session["created_at"]).split('.')[0]
       }
   
   def get_session_knowledge(self, session_id: str) -> Dict:
       """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π —Å–µ—Å—Å–∏–∏ –¥–ª—è –≤–∫–ª–∞–¥–∫–∏ '–ó–Ω–∞–Ω–∏—è'."""
       session = self.get_or_create_session(session_id)
       
       knowledge_data = {
           "local_knowledge": session["local_knowledge"],
           "stm_items": [],
           "ltm_items": [],
           "stats": {
               "total_knowledge": len(session["local_knowledge"]),
               "active_memories": 0,
               "graph_nodes": 0,
               "connections": 0
           }
       }
       
       # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ NeuroGraph –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
       if self.memory:
           try:
               stats = self.memory.get_memory_statistics()
               knowledge_data["stats"].update({
                   "active_memories": stats['memory_levels']['stm']['size'],
                   "graph_nodes": stats['memory_levels']['ltm']['size'],
                   "total_knowledge": stats.get('total_items', len(session["local_knowledge"]))
               })
               
               # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏–∑ –ø–∞–º—è—Ç–∏
               try:
                   recent_items = self.memory.get_recent_items()  # –£–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä limit
                   knowledge_data["stm_items"] = [
                       {
                           "content": getattr(item, 'content', str(item))[:100] + ("..." if len(str(getattr(item, 'content', str(item)))) > 100 else ""),
                           "tags": getattr(item, 'metadata', {}).get('tags', ['–ø–∞–º—è—Ç—å'])
                       }
                       for item in (recent_items[:5] if recent_items else [])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 5 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                   ]
               except Exception as e:
                   print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è recent_items: {e}")
                   knowledge_data["stm_items"] = []
               
               # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ LTM
               try:
                   # –ü–æ–ª—É—á–∞–µ–º –¥–æ—Å—Ç—É–ø –∫ LTM –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É
                   if hasattr(self.memory, 'ltm') and self.memory.ltm:
                       ltm_items = []
                       # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —ç–ª–µ–º–µ–Ω—Ç—ã –∏–∑ LTM
                       if hasattr(self.memory.ltm, 'get_all_items'):
                           ltm_all = self.memory.ltm.get_all_items()
                           ltm_items = ltm_all[:5] if ltm_all else []
                       elif hasattr(self.memory.ltm, 'items'):
                           ltm_items = list(self.memory.ltm.items())[:5]
                       
                       knowledge_data["ltm_items"] = [
                           {
                               "content": getattr(item, 'content', str(item))[:100] + ("..." if len(str(getattr(item, 'content', str(item)))) > 100 else ""),
                               "tags": getattr(item, 'metadata', {}).get('tags', ['–¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è'])
                           }
                           for item in ltm_items
                       ]
                   else:
                       knowledge_data["ltm_items"] = []
                       
               except Exception as e:
                   print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è LTM –¥–∞–Ω–Ω—ã—Ö: {e}")
                   knowledge_data["ltm_items"] = []
               
           except Exception as e:
               print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–∞–º—è—Ç–∏: {e}")
               knowledge_data["stats"]["error"] = str(e)
       
       return knowledge_data
   
   def get_system_status(self) -> Dict:
       """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ —Å—Ç–∞—Ç—É—Å–∞ –¥–ª—è –≤–∫–ª–∞–¥–∫–∏ '–°–∏—Å—Ç–µ–º–∞'."""
       uptime = datetime.now() - self.global_stats["start_time"]
       
       status = {
           "components": {
               "nlp": {"status": "active" if self.engine else "inactive", "performance": 85},
               "semgraph": {"status": "active" if self.graph else "inactive", "performance": 92},
               "memory": {"status": "active" if self.memory else "inactive", "performance": 78},
               "processor": {"status": "active" if self.engine else "inactive", "performance": 88}
           },
           "stats": {
               "uptime": str(uptime).split('.')[0],
               "total_users": self.global_stats["total_users"],
               "total_messages": self.global_stats["total_messages"],
               "avg_response_time": "0.8s",
               "system_health": "healthy" if self.global_stats["system_healthy"] else "degraded"
           },
           "memory_stats": {}
       }
       
       if self.memory:
           try:
               memory_stats = self.memory.get_memory_statistics()
               status["memory_stats"] = memory_stats
           except Exception as e:
               status["memory_stats"] = {"error": str(e)}
       
       return status
   
   async def _async_delay(self, seconds: float):
       """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
       import asyncio
       await asyncio.sleep(seconds)

# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
assistant = WebNeuroGraphAssistant()

@app.route('/')
def index():
   """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞."""
   return render_template('neurograph_assistant.html')

@app.route('/api/message', methods=['POST'])
def handle_message():
   """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
   try:
       data = request.get_json()
       message = data.get('message', '').strip()
       session_id = data.get('session_id', str(uuid.uuid4()))
       
       if not message:
           return jsonify({"error": "–ü—É—Å—Ç–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ"}), 400
       
       # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã)
       import asyncio
       loop = asyncio.new_event_loop()
       asyncio.set_event_loop(loop)
       result = loop.run_until_complete(assistant.process_message(session_id, message))
       loop.close()
       
       return jsonify(result)
       
   except Exception as e:
       return jsonify({"error": str(e)}), 500

@app.route('/api/bulk/text', methods=['POST'])
def bulk_learn_text():
   """–ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –±–æ–ª—å—à–æ–º —Ç–µ–∫—Å—Ç–µ."""
   try:
       data = request.get_json()
       text = data.get('text', '').strip()
       session_id = data.get('session_id', str(uuid.uuid4()))
       
       if not text:
           return jsonify({"error": "–¢–µ–∫—Å—Ç –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω"}), 400
       
       if len(text) < 100:
           return jsonify({"error": "–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (–º–∏–Ω–∏–º—É–º 100 —Å–∏–º–≤–æ–ª–æ–≤)"}), 400
       
       # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
       chunk_size = data.get('chunk_size', 500)
       importance = data.get('importance', 0.7)
       category = data.get('category', 'bulk_text')
       
       # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞–∫–µ—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
       result = assistant.process_bulk_text(
           text=text,
           session_id=session_id,
           chunk_size=chunk_size,
           importance=importance,
           category=category
       )
       
       return jsonify(result)
       
   except Exception as e:
       return jsonify({"error": str(e)}), 500

@app.route('/api/bulk/file', methods=['POST'])
def bulk_learn_file():
   """–ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ."""
   try:
       if 'file' not in request.files:
           return jsonify({"error": "–§–∞–π–ª –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω"}), 400
       
       file = request.files['file']
       if file.filename == '':
           return jsonify({"error": "–§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω"}), 400
       
       # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞
       allowed_extensions = {'.txt', '.md', '.py', '.js', '.html', '.json', '.csv'}
       file_ext = os.path.splitext(file.filename)[1].lower()
       
       if file_ext not in allowed_extensions:
           return jsonify({"error": f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞: {file_ext}"}), 400
       
       # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
       try:
           if file_ext == '.csv':
               file_content = file.read().decode('utf-8')
           else:
               file_content = file.read().decode('utf-8')
       except UnicodeDecodeError:
           try:
               file_content = file.read().decode('cp1251')  # –ü–æ–ø—Ä–æ–±—É–µ–º windows-1251
           except UnicodeDecodeError:
               return jsonify({"error": "–ù–µ —É–¥–∞–ª–æ—Å—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª"}), 400
       
       if len(file_content) < 100:
           return jsonify({"error": "–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –º–∞–ª –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"}), 400
       
       # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
       session_id = request.form.get('session_id', str(uuid.uuid4()))
       chunk_size = int(request.form.get('chunk_size', 500))
       importance = float(request.form.get('importance', 0.7))
       
       # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–∞
       result = assistant.process_bulk_file(
           file_content=file_content,
           filename=file.filename,
           session_id=session_id,
           chunk_size=chunk_size,
           importance=importance
       )
       
       return jsonify(result)
       
   except Exception as e:
       return jsonify({"error": str(e)}), 500

@app.route('/api/bulk/task/<task_id>')
def get_bulk_task_status(task_id):
   """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è."""
   try:
       result = assistant.get_bulk_task_status(task_id)
       return jsonify(result)
   except Exception as e:
       return jsonify({"error": str(e)}), 500

@app.route('/api/bulk/progress')
def get_bulk_progress():
   """–ü–æ–ª—É—á–µ–Ω–∏–µ –ª–æ–≥–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è."""
   try:
       limit = request.args.get('limit', 50, type=int)
       progress_log = assistant.get_bulk_progress_log(limit)
       return jsonify({
           "success": True,
           "progress": progress_log,
           "total_entries": len(progress_log)
       })
   except Exception as e:
       return jsonify({"error": str(e)}), 500

@app.route('/api/bulk/tasks/<session_id>')
def get_session_bulk_tasks(session_id):
   """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –∑–∞–¥–∞—á –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Å–µ—Å—Å–∏–∏."""
   try:
       session_tasks = {}
       for task_id, task_info in assistant.bulk_tasks.items():
           if task_info.get('session_id') == session_id:
               session_tasks[task_id] = {
                   'status': task_info['status'],
                   'start_time': task_info['start_time'].isoformat(),
                   'filename': task_info.get('filename', '–¢–µ–∫—Å—Ç'),
               }
               if 'end_time' in task_info:
                   session_tasks[task_id]['end_time'] = task_info['end_time'].isoformat()
               if 'result' in task_info:
                   session_tasks[task_id]['summary'] = {
                       'total_chunks': task_info['result'].get('total_chunks', 0),
                       'processed_chunks': task_info['result'].get('processed_chunks', 0),
                       'processing_time': task_info['result'].get('processing_time_seconds', 0)
                   }
       
       return jsonify({
           "success": True,
           "tasks": session_tasks
       })
   except Exception as e:
       return jsonify({"error": str(e)}), 500

@app.route('/api/bulk/export/<task_id>')
def export_bulk_task_report(task_id):
   """–≠–∫—Å–ø–æ—Ä—Ç –æ—Ç—á–µ—Ç–∞ –æ –∑–∞–¥–∞—á–µ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è."""
   try:
       if task_id not in assistant.bulk_tasks:
           return jsonify({"error": "–ó–∞–¥–∞—á–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"}), 404
       
       task_info = assistant.bulk_tasks[task_id]
       
       # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å –æ—Ç—á–µ—Ç–æ–º
       report = {
           "task_id": task_id,
           "export_time": datetime.now().isoformat(),
           "task_info": {
               "status": task_info['status'],
               "start_time": task_info['start_time'].isoformat(),
               "session_id": task_info['session_id']
           }
       }
       
       if 'end_time' in task_info:
           report["task_info"]["end_time"] = task_info['end_time'].isoformat()
       
       if 'result' in task_info:
           report["result"] = task_info['result']
       
       if 'error' in task_info:
           report["error"] = task_info['error']
       
       # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
       temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8')
       json.dump(report, temp_file, indent=2, ensure_ascii=False)
       temp_file.close()
       
       filename = f"bulk_learning_report_{task_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
       
       return send_file(
           temp_file.name,
           as_attachment=True,
           download_name=filename,
           mimetype='application/json'
       )
       
   except Exception as e:
       return jsonify({"error": str(e)}), 500

@app.route('/api/bulk/cleanup', methods=['POST'])
def cleanup_bulk_tasks():
   """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∑–∞–¥–∞—á –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è."""
   try:
       data = request.get_json() or {}
       max_age_hours = data.get('max_age_hours', 24)
       
       removed_count = assistant.cleanup_old_tasks(max_age_hours)
       
       return jsonify({
           "success": True,
           "removed_tasks": removed_count,
           "remaining_tasks": len(assistant.bulk_tasks)
       })
       
   except Exception as e:
       return jsonify({"error": str(e)}), 500

@app.route('/api/knowledge/<session_id>')
def get_knowledge(session_id):
   """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –¥–ª—è –≤–∫–ª–∞–¥–∫–∏ '–ó–Ω–∞–Ω–∏—è'."""
   try:
       knowledge = assistant.get_session_knowledge(session_id)
       return jsonify(knowledge)
   except Exception as e:
       return jsonify({"error": str(e)}), 500

@app.route('/api/system')
def get_system_status():
   """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ —Å—Ç–∞—Ç—É—Å–∞."""
   try:
       status = assistant.get_system_status()
       return jsonify(status)
   except Exception as e:
       return jsonify({"error": str(e)}), 500

@app.route('/api/sessions/<session_id>')
def get_session_info(session_id):
   """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–µ—Å—Å–∏–∏."""
   try:
       session = assistant.get_or_create_session(session_id)
       stats = assistant._get_session_stats(session)
       return jsonify({
           "session": stats,
           "messages": session["messages"][-50:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 50 —Å–æ–æ–±—â–µ–Ω–∏–π
       })
   except Exception as e:
       return jsonify({"error": str(e)}), 500

@app.route('/api/health')
def health_check():
   """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã."""
   return jsonify({
       "status": "healthy" if assistant.global_stats["system_healthy"] else "degraded",
       "neurograph_available": NEUROGRAPH_AVAILABLE,
       "bulk_learning_available": BULK_LEARNING_AVAILABLE,
       "active_sessions": len(assistant.user_sessions),
       "active_bulk_tasks": len(assistant.bulk_tasks) if hasattr(assistant, 'bulk_tasks') else 0,
       "uptime": str(datetime.now() - assistant.global_stats["start_time"]).split('.')[0]
   })

if __name__ == '__main__':
   # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É templates –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
   if not os.path.exists('templates'):
       os.makedirs('templates')
   
   print("üöÄ –ó–∞–ø—É—Å–∫ NeuroGraph Web Assistant")
   print("=" * 50)
   print(f"NeuroGraph –¥–æ—Å—Ç—É–ø–µ–Ω: {NEUROGRAPH_AVAILABLE}")
   print(f"–ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ: {BULK_LEARNING_AVAILABLE}")
   print(f"–ê–∫—Ç–∏–≤–Ω—ã—Ö —Å–µ—Å—Å–∏–π: {len(assistant.user_sessions)}")
   print("–í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É: http://localhost:5000")
   print("=" * 50)
   
   app.run(debug=True, host='0.0.0.0', port=5000)