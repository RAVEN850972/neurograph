import os
import json
import time
import uuid
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable
from datetime import datetime
import threading
import queue

class TextChunker:
    """–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ —á–∞—Å—Ç–∏."""
    
    def __init__(self, 
                 chunk_size: int = 500, 
                 overlap: int = 50,
                 respect_sentences: bool = True,
                 respect_paragraphs: bool = True):
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.respect_sentences = respect_sentences
        self.respect_paragraphs = respect_paragraphs
    
    def chunk_text(self, text: str, metadata: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ —á–∞—Å—Ç–∏."""
        if not text.strip():
            return []
        
        chunks = []
        metadata = metadata or {}
        
        if self.respect_paragraphs:
            # –°–Ω–∞—á–∞–ª–∞ —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º
            paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
            
            for para_idx, paragraph in enumerate(paragraphs):
                para_chunks = self._chunk_paragraph(paragraph, para_idx, len(paragraphs))
                
                for chunk_data in para_chunks:
                    chunk_data['metadata'].update(metadata)
                    chunks.append(chunk_data)
        else:
            # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Ä–∞–∑–º–µ—Ä—É
            simple_chunks = self._simple_chunk(text)
            for i, chunk_text in enumerate(simple_chunks):
                chunks.append({
                    'content': chunk_text,
                    'chunk_id': str(uuid.uuid4()),
                    'chunk_index': i,
                    'total_chunks': len(simple_chunks),
                    'metadata': {**metadata, 'chunk_type': 'simple'}
                })
        
        return chunks
    
    def _chunk_paragraph(self, paragraph: str, para_idx: int, total_paras: int) -> List[Dict[str, Any]]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ –Ω–∞ —á–∞—Å—Ç–∏."""
        chunks = []
        
        if len(paragraph) <= self.chunk_size:
            # –ü–∞—Ä–∞–≥—Ä–∞—Ñ –ø–æ–º–µ—â–∞–µ—Ç—Å—è —Ü–µ–ª–∏–∫–æ–º
            chunks.append({
                'content': paragraph,
                'chunk_id': str(uuid.uuid4()),
                'chunk_index': 0,
                'paragraph_index': para_idx,
                'total_paragraphs': total_paras,
                'metadata': {'chunk_type': 'paragraph', 'is_complete_paragraph': True}
            })
        else:
            # –†–∞–∑–±–∏–≤–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ –Ω–∞ —á–∞—Å—Ç–∏
            if self.respect_sentences:
                sentences = self._split_sentences(paragraph)
                sentence_chunks = self._group_sentences(sentences)
            else:
                sentence_chunks = self._simple_chunk(paragraph)
            
            for i, chunk_text in enumerate(sentence_chunks):
                chunks.append({
                    'content': chunk_text,
                    'chunk_id': str(uuid.uuid4()),
                    'chunk_index': i,
                    'paragraph_index': para_idx,
                    'total_paragraphs': total_paras,
                    'total_chunks_in_paragraph': len(sentence_chunks),
                    'metadata': {'chunk_type': 'paragraph_part', 'is_complete_paragraph': False}
                })
        
        return chunks
    
    def _split_sentences(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."""
        import re
        
        # –ü—Ä–æ—Å—Ç–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]+\s+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        return sentences
    
    def _group_sentences(self, sentences: List[str]) -> List[str]:
        """–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ —á–∞—Å—Ç–∏ –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞."""
        chunks = []
        current_chunk = []
        current_length = 0
        
        for sentence in sentences:
            sentence_length = len(sentence)
            
            if current_length + sentence_length > self.chunk_size and current_chunk:
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é —á–∞—Å—Ç—å
                chunks.append(' '.join(current_chunk))
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é —á–∞—Å—Ç—å —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
                if self.overlap > 0 and len(current_chunk) > 1:
                    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
                    current_chunk = [current_chunk[-1], sentence]
                    current_length = len(current_chunk[-2]) + sentence_length
                else:
                    current_chunk = [sentence]
                    current_length = sentence_length
            else:
                current_chunk.append(sentence)
                current_length += sentence_length
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —á–∞—Å—Ç—å
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks
    
    def _simple_chunk(self, text: str) -> List[str]:
        """–ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Ä–∞–∑–º–µ—Ä—É —Å–∏–º–≤–æ–ª–æ–≤."""
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.chunk_size
            
            if end >= len(text):
                chunks.append(text[start:])
                break
            
            # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π –ø—Ä–æ–±–µ–ª –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Ä–∞–∑—Ä—ã–≤–∞
            if end < len(text):
                while end > start and text[end] != ' ':
                    end -= 1
                
                if end == start:  # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø—Ä–æ–±–µ–ª
                    end = start + self.chunk_size
            
            chunks.append(text[start:end])
            start = end - self.overlap if self.overlap > 0 else end
        
        return chunks


class BulkLearningSystem:
    """–°–∏—Å—Ç–µ–º–∞ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è NeuroGraph –Ω–∞ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–∞—Ö –¥–∞–Ω–Ω—ã—Ö."""
    
    def __init__(self, neurograph_engine, 
                 batch_size: int = 10,
                 delay_between_batches: float = 0.5,
                 progress_callback: Optional[Callable] = None):
        self.engine = neurograph_engine
        self.batch_size = batch_size
        self.delay_between_batches = delay_between_batches
        self.progress_callback = progress_callback
        
        self.chunker = TextChunker()
        self.learning_stats = {
            'total_processed': 0,
            'successful': 0,
            'failed': 0,
            'start_time': None,
            'end_time': None,
            'errors': []
        }
        
        # –î–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self._processing_queue = queue.Queue()
        self._results_queue = queue.Queue()
        self._is_processing = False
        self._worker_thread = None
    
    def process_text_file(self, file_path: str, 
                         encoding: str = 'utf-8',
                         category: str = 'document',
                         importance: float = 0.7) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª."""
        
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                text = f.read()
            
            metadata = {
                'source_file': os.path.basename(file_path),
                'source_path': file_path,
                'file_size': len(text),
                'category': category,
                'processed_at': datetime.now().isoformat()
            }
            
            return self.process_large_text(text, metadata, importance)
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'file_path': file_path
            }
    
    def process_multiple_files(self, file_paths: List[str], 
                              encoding: str = 'utf-8',
                              category: str = 'documents') -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤."""
        
        results = {
            'total_files': len(file_paths),
            'processed_files': 0,
            'failed_files': 0,
            'results': [],
            'errors': []
        }
        
        for file_path in file_paths:
            try:
                self._report_progress(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {os.path.basename(file_path)}")
                
                result = self.process_text_file(file_path, encoding, category)
                results['results'].append(result)
                
                if result.get('success', False):
                    results['processed_files'] += 1
                else:
                    results['failed_files'] += 1
                    results['errors'].append({
                        'file': file_path,
                        'error': result.get('error', 'Unknown error')
                    })
                
                # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Ñ–∞–π–ª–∞–º–∏
                time.sleep(self.delay_between_batches)
                
            except Exception as e:
                results['failed_files'] += 1
                results['errors'].append({
                    'file': file_path,
                    'error': str(e)
                })
        
        return results
    
    def process_large_text(self, text: str, 
                          metadata: Dict[str, Any] = None,
                          importance: float = 0.7) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–æ–ª—å—à–æ–π —Ç–µ–∫—Å—Ç, —Ä–∞–∑–±–∏–≤–∞—è –µ–≥–æ –Ω–∞ —á–∞—Å—Ç–∏."""
        
        self.learning_stats['start_time'] = datetime.now()
        metadata = metadata or {}
        
        try:
            # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏
            chunks = self.chunker.chunk_text(text, metadata)
            
            self._report_progress(f"–¢–µ–∫—Å—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(chunks)} —á–∞—Å—Ç–µ–π")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–∏ –ø–∞–∫–µ—Ç–∞–º–∏
            results = self._process_chunks_in_batches(chunks, importance)
            
            self.learning_stats['end_time'] = datetime.now()
            processing_time = (self.learning_stats['end_time'] - self.learning_stats['start_time']).total_seconds()
            
            return {
                'success': True,
                'total_chunks': len(chunks),
                'processed_chunks': results['successful'],
                'failed_chunks': results['failed'],
                'processing_time_seconds': processing_time,
                'chunks_per_second': len(chunks) / processing_time if processing_time > 0 else 0,
                'stats': self.learning_stats,
                'sample_chunks': chunks[:3] if chunks else []  # –ü–µ—Ä–≤—ã–µ 3 —á–∞—Å—Ç–∏ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
            }
            
        except Exception as e:
            self.learning_stats['end_time'] = datetime.now()
            return {
                'success': False,
                'error': str(e),
                'stats': self.learning_stats
            }
    
    def _process_chunks_in_batches(self, chunks: List[Dict], importance: float) -> Dict[str, int]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —á–∞—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ –ø–∞–∫–µ—Ç–∞–º–∏."""
        
        results = {'successful': 0, 'failed': 0}
        
        for i in range(0, len(chunks), self.batch_size):
            batch = chunks[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1
            total_batches = (len(chunks) + self.batch_size - 1) // self.batch_size
            
            self._report_progress(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞ {batch_num}/{total_batches} ({len(batch)} —á–∞—Å—Ç–µ–π)")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞–∫–µ—Ç
            batch_results = self._process_batch(batch, importance)
            results['successful'] += batch_results['successful']
            results['failed'] += batch_results['failed']
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–∞–∫–µ—Ç–∞–º–∏
            if i + self.batch_size < len(chunks):
                time.sleep(self.delay_between_batches)
        
        return results
    
    def _process_batch(self, batch: List[Dict], importance: float) -> Dict[str, int]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –ø–∞–∫–µ—Ç —á–∞—Å—Ç–µ–π."""
        
        results = {'successful': 0, 'failed': 0}
        
        for chunk_data in batch:
            try:
                # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
                content = chunk_data['content']
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                context_info = []
                if 'paragraph_index' in chunk_data:
                    context_info.append(f"–ü–∞—Ä–∞–≥—Ä–∞—Ñ {chunk_data['paragraph_index'] + 1}")
                if 'chunk_index' in chunk_data:
                    context_info.append(f"–ß–∞—Å—Ç—å {chunk_data['chunk_index'] + 1}")
                
                if context_info:
                    content = f"[{', '.join(context_info)}] {content}"
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
                source = chunk_data['metadata'].get('source_file', 'bulk_text')
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–≥–∏
                tags = ['bulk_learning']
                if 'category' in chunk_data['metadata']:
                    tags.append(chunk_data['metadata']['category'])
                if 'chunk_type' in chunk_data['metadata']:
                    tags.append(chunk_data['metadata']['chunk_type'])
                
                # –û–±—É—á–∞–µ–º NeuroGraph
                response = self.engine.learn(content)
                
                if hasattr(response, 'success') and response.success:
                    results['successful'] += 1
                    self.learning_stats['successful'] += 1
                else:
                    results['failed'] += 1
                    self.learning_stats['failed'] += 1
                    error_msg = getattr(response, 'error_message', 'Unknown error')
                    self.learning_stats['errors'].append({
                        'chunk_id': chunk_data['chunk_id'],
                        'error': error_msg
                    })
                
                self.learning_stats['total_processed'] += 1
                
            except Exception as e:
                results['failed'] += 1
                self.learning_stats['failed'] += 1
                self.learning_stats['total_processed'] += 1
                self.learning_stats['errors'].append({
                    'chunk_id': chunk_data.get('chunk_id', 'unknown'),
                    'error': str(e)
                })
        
        return results
    
    def _report_progress(self, message: str):
        """–°–æ–æ–±—â–∞–µ—Ç –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ."""
        if self.progress_callback:
            self.progress_callback(message)
        else:
            print(f"[{datetime.now().strftime('%H:%M:%S')}] {message}")
    
    def process_directory(self, directory_path: str, 
                         file_extensions: List[str] = ['.txt', '.md'],
                         recursive: bool = True,
                         encoding: str = 'utf-8') -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        
        directory = Path(directory_path)
        if not directory.exists():
            return {
                'success': False,
                'error': f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {directory_path}"
            }
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Ñ–∞–π–ª—ã
        files = []
        if recursive:
            for ext in file_extensions:
                files.extend(directory.rglob(f"*{ext}"))
        else:
            for ext in file_extensions:
                files.extend(directory.glob(f"*{ext}"))
        
        file_paths = [str(f) for f in files]
        
        self._report_progress(f"–ù–∞–π–¥–µ–Ω–æ {len(file_paths)} —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        if not file_paths:
            return {
                'success': False,
                'error': '–§–∞–π–ª—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã'
            }
        
        return self.process_multiple_files(file_paths, encoding, 'directory_files')
    
    def export_learning_report(self, output_file: str) -> bool:
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ–± –æ–±—É—á–µ–Ω–∏–∏."""
        
        try:
            report = {
                'learning_session': {
                    'start_time': self.learning_stats['start_time'].isoformat() if self.learning_stats['start_time'] else None,
                    'end_time': self.learning_stats['end_time'].isoformat() if self.learning_stats['end_time'] else None,
                    'duration_seconds': (
                        self.learning_stats['end_time'] - self.learning_stats['start_time']
                    ).total_seconds() if self.learning_stats['start_time'] and self.learning_stats['end_time'] else None
                },
                'statistics': self.learning_stats,
                'configuration': {
                    'batch_size': self.batch_size,
                    'delay_between_batches': self.delay_between_batches,
                    'chunk_size': self.chunker.chunk_size,
                    'overlap': self.chunker.overlap,
                    'respect_sentences': self.chunker.respect_sentences,
                    'respect_paragraphs': self.chunker.respect_paragraphs
                }
            }
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            return True
            
        except Exception as e:
            self._report_progress(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ –æ—Ç—á–µ—Ç–∞: {e}")
            return False


class AsyncBulkLearning:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è."""
    
    def __init__(self, neurograph_engine, max_concurrent: int = 3):
        self.engine = neurograph_engine
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        
    async def process_large_text_async(self, text: str, 
                                     metadata: Dict[str, Any] = None,
                                     chunk_size: int = 500,
                                     importance: float = 0.7) -> Dict[str, Any]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
        
        chunker = TextChunker(chunk_size=chunk_size)
        chunks = chunker.chunk_text(text, metadata or {})
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞—Å—Ç–µ–π
        tasks = []
        for chunk_data in chunks:
            task = self._process_chunk_async(chunk_data, importance)
            tasks.append(task)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–¥–∞—á–∏
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        successful = sum(1 for r in results if r is True)
        failed = len(results) - successful
        
        return {
            'success': True,
            'total_chunks': len(chunks),
            'successful': successful,
            'failed': failed,
            'results': results
        }
    
    async def _process_chunk_async(self, chunk_data: Dict, importance: float) -> bool:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π —á–∞—Å—Ç–∏."""
        
        async with self.semaphore:
            try:
                # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã await –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ NeuroGraph API
                # –ü–æ–∫–∞ –¥–µ–ª–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ –≤ executor
                loop = asyncio.get_event_loop()
                response = await loop.run_in_executor(
                    None, 
                    self.engine.learn, 
                    chunk_data['content']
                )
                response = self.engine.learn(content)
                print(f"[DEBUG] –û–±—É—á–µ–Ω–∏–µ —á–∞—Å—Ç–∏ {chunk_data['chunk_id']}: success={hasattr(response, 'success') and response.success}, response={response}")
                return hasattr(response, 'success') and response.success
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞—Å—Ç–∏ {chunk_data.get('chunk_id', 'unknown')}: {e}")
                return False


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def example_usage():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è."""
    
    try:
        from neurograph.integration import create_default_engine
        
        # –°–æ–∑–¥–∞–µ–º –¥–≤–∏–∂–æ–∫
        engine = create_default_engine()
        
        # –°–æ–∑–¥–∞–µ–º —Å–∏—Å—Ç–µ–º—É –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        bulk_system = BulkLearningSystem(
            neurograph_engine=engine,
            batch_size=5,  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ 5 —á–∞—Å—Ç–µ–π –∑–∞ —Ä–∞–∑
            delay_between_batches=0.2  # –ü–∞—É–∑–∞ 200–º—Å –º–µ–∂–¥—É –ø–∞–∫–µ—Ç–∞–º–∏
        )
        
        # –ü—Ä–∏–º–µ—Ä 1: –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        large_text = """
        –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç (–ò–ò) ‚Äî —ç—Ç–æ –æ–±–ª–∞—Å—Ç—å –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –Ω–∞—É–∫, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è —Å–æ–∑–¥–∞–Ω–∏–µ–º –º–∞—à–∏–Ω, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏, –æ–±—ã—á–Ω–æ —Ç—Ä–µ–±—É—é—â–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.
        
        –ò—Å—Ç–æ—Ä–∏—è –ò–ò –Ω–∞—á–∞–ª–∞—Å—å –≤ 1950-—Ö –≥–æ–¥–∞—Ö, –∫–æ–≥–¥–∞ —É—á–µ–Ω—ã–µ –≤–ø–µ—Ä–≤—ã–µ –Ω–∞—á–∞–ª–∏ —Å–µ—Ä—å–µ–∑–Ω–æ –∏–∑—É—á–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Å–æ–∑–¥–∞–Ω–∏—è –¥—É–º–∞—é—â–∏—Ö –º–∞—à–∏–Ω. –ê–ª–∞–Ω –¢—å—é—Ä–∏–Ω–≥ –ø—Ä–µ–¥–ª–æ–∂–∏–ª –∑–Ω–∞–º–µ–Ω–∏—Ç—ã–π —Ç–µ—Å—Ç –¢—å—é—Ä–∏–Ω–≥–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–∞—à–∏–Ω—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ.
        
        –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ò–ò –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø–æ–¥—Ö–æ–¥–æ–≤, –≤–∫–ª—é—á–∞—è –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –∏ —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ. –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º —É—á–∏—Ç—å—Å—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ —è–≤–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è.
        
        –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ò–ò –≤–∫–ª—é—á–∞—é—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏, –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ, –æ–±—Ä–∞–±–æ—Ç–∫—É –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫—É –∏ –º–Ω–æ–≥–æ–µ –¥—Ä—É–≥–æ–µ. –ò–ò —É–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö, —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö, –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤–∞—Ö –∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–µ.
        
        –ë—É–¥—É—â–µ–µ –ò–ò –æ–±–µ—â–∞–µ—Ç –µ—â–µ –±–æ–ª–µ–µ —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Ç–æ–º, –∫–∞–∫ –º—ã —Ä–∞–±–æ—Ç–∞–µ–º, –æ–±—â–∞–µ–º—Å—è –∏ –∂–∏–≤–µ–º. –û–¥–Ω–∞–∫–æ —ç—Ç–æ —Ç–∞–∫–∂–µ –ø–æ–¥–Ω–∏–º–∞–µ—Ç –≤–∞–∂–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã —ç—Ç–∏–∫–∏, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –≤–ª–∏—è–Ω–∏—è –Ω–∞ –æ–±—â–µ—Å—Ç–≤–æ.
        """ * 3  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –≤ 3 —Ä–∞–∑–∞
        
        print("üöÄ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–æ–≥–æ —Ç–µ–∫—Å—Ç–∞...")
        result = bulk_system.process_large_text(
            text=large_text,
            metadata={'topic': 'AI', 'source': 'example'},
            importance=0.8
        )
        
        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
        print(f"  –í—Å–µ–≥–æ —á–∞—Å—Ç–µ–π: {result['total_chunks']}")
        print(f"  –£—Å–ø–µ—à–Ω–æ: {result['processed_chunks']}")
        print(f"  –û—à–∏–±–æ–∫: {result['failed_chunks']}")
        print(f"  –í—Ä–µ–º—è: {result['processing_time_seconds']:.2f} —Å–µ–∫")
        print(f"  –°–∫–æ—Ä–æ—Å—Ç—å: {result['chunks_per_second']:.2f} —á–∞—Å—Ç–µ–π/—Å–µ–∫")
        
        # –ü—Ä–∏–º–µ—Ä 2: –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ –∏ –µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞
        test_file = "test_document.txt"
        with open(test_file, 'w', encoding='utf-8') as f:
            f.write(large_text)
        
        print(f"\nüìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ {test_file}...")
        file_result = bulk_system.process_text_file(test_file, category='test_document')
        
        if file_result['success']:
            print(f"‚úÖ –§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω!")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {file_result['error']}")
        
        # –≠–∫—Å–ø–æ—Ä—Ç –æ—Ç—á–µ—Ç–∞
        bulk_system.export_learning_report("learning_report.json")
        print("üìä –û—Ç—á–µ—Ç –æ–± –æ–±—É—á–µ–Ω–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ learning_report.json")
        
        # –û—á–∏—Å—Ç–∫–∞
        if os.path.exists(test_file):
            os.remove(test_file)
        
        # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
        engine.shutdown()
        print("‚úÖ –ü—Ä–∏–º–µ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω!")
        
    except ImportError:
        print("‚ùå NeuroGraph –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º NeuroGraph.")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    example_usage()